---
categories:
  - 科研启蒙
  - CVLearning
date: 2025-09-20 00:00:00
article: true
---
## Optimizer: 优化器
### SGD（Stochastic gradient descent）
(Chinese Name: 随机梯度下降法)
The main idea of SGD can be explained by the following formula:
$$
W \leftarrow W - \eta \frac{\partial L}{\partial W}
$$
where $\eta$ stands for learning rate, which is ussally set to be predefined values like 0.001 or 0.01. 

```python
class SGD:
	def __init__(self, lr=0.01):
		self.lr=lr
	
	def update(self, params, grads):
		for key in params.keys():
			params[key]+=lr* grads[key]
```

SGD is easy to complement, but sucks at its efficiency. In an example of function $f(x,y)= \frac{1}{20}x^2+ y^2$
![image.png](https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/20250922223004.png) the gradient of the function is nearly parallel to the y axis, which means it can be hard to find the "best position" of the function

