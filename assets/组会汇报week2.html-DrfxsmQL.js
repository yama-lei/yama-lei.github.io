import{_ as t,c as l,d as e,b as a,e as o,g as p,f as s,r,o as g}from"./app-iw0frhaY.js";const c={};function d(m,i){const n=r("center");return g(),l("div",null,[i[5]||(i[5]=e(`<h1 id="例会汇报-第二次" tabindex="-1"><a class="header-anchor" href="#例会汇报-第二次"><span>例会汇报 | 第二次</span></a></h1><p>（接上一次BLIP系列模型的讨论）我没有仔细看对应的论文，下方图片来自知乎。</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/6fb85be2a08f15ce71245f1301d3b1ab.png" alt="6fb85be2a08f15ce71245f1301d3b1ab" tabindex="0" loading="lazy"><figcaption>6fb85be2a08f15ce71245f1301d3b1ab</figcaption></figure><hr><h4 id="一个vqa领域的baseline模型" tabindex="-1"><a class="header-anchor" href="#一个vqa领域的baseline模型"><span>一个VQA领域的baseline模型</span></a></h4><blockquote><p>Baseline模型的含义： 容易实现、功能基础的模型，作为&#39;基线&#39;(baseline)。</p></blockquote><p>我找到一篇10年前的论文：<a href="paper.html">Simple Baseline for Visual Question Answering</a>，文章中提到的“iBowing”模型的结构是：</p><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" data-title="mermaid" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">graph TD</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A[输入问题（文本）] --&gt;|One-hot Encoding + 词袋模型（BoW）| B[文本特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    C[输入图像] --&gt;|CNN（如GoogLeNet）| D[图像特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    B --&gt;|拼接| E[联合特征向量]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    D --&gt;|拼接| E</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    E --&gt;|Softmax 分类| F[预测答案]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>模型核心思路如下：</p>`,9)),a("ol",null,[i[3]||(i[3]=e("<li><p><strong>文字处理</strong></p><ul><li><strong>One-hot Encoding 与 Bag-of-Words (BoW)：</strong><br> 模型首先将输入的问句转换为 one-hot 向量。在 one-hot encoding 中，每个单词都表示为一个二值向量（只有对应单词的位置为 1，其余位置为 0）。这种方法正是实现了所谓的“词袋模型”（Bag-of-Words），该模型只关注词汇的出现频率而忽略词序。</li></ul><blockquote><p>Bow（词袋模型）是只统计单词在词汇表中的出现情况，忽略了单词的语义关系和次序关系。</p><p><strong>但是</strong>，这个模型并没有直接使用one-hot向量，而是通入一个嵌入层，实现词嵌入: The input question is first converted to a one-hot vector, which is transformed to word feature via a word embedding layer</p></blockquote><ul><li><strong>Word Embedding（词嵌入）：</strong><br> 将 one-hot 向量输入到词嵌入层中，转换为低维稠密向量。词嵌入能够捕捉单词之间的语义关系，比简单的频数统计提供了更丰富的语义信息。</li></ul></li><li><p><strong>图像处理</strong></p><ul><li>利用预训练 CNN（例如 GoogLeNet）提取图像深度特征，获得图像的高层语义信息。</li></ul></li>",2)),a("li",null,[i[1]||(i[1]=e("<p><strong>特征拼接（Concatenation）：</strong><br> 将文字特征和图像特征直接进行拼接，即将两个向量<strong>横向连接</strong>，形成一个联合特征向量。这种拼接方式能够同时包含问题的文本信息和图像的视觉信息，为后续的分类提供全面的输入特征。</p><blockquote><p>这里的<strong>拼接</strong>是这个含义：</p><ul><li><strong>文本特征向量</strong>：<code>T = [0.2, 0.5, 0.3]</code> （假设 3 维向量）</li><li><strong>图像特征向量</strong>：<code>I = [0.7, 0.1, 0.9, 0.4]</code> （假设 4 维特征）</li></ul><p>那么，拼接后的 <strong>联合特征向量</strong> <code>F</code> 就是：F=[0.2,0.5,0.3,0.7,0.1,0.9,0.4]</p></blockquote>",2)),o(n,null,{default:p(()=>i[0]||(i[0]=[s("图片来自论文：https://arxiv.org/abs/1512.02167")])),_:1}),i[2]||(i[2]=a("figure",null,[a("img",{src:"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/x1.png",alt:"Refer to caption",tabindex:"0",loading:"lazy"}),a("figcaption",null,"Refer to caption")],-1))]),i[4]||(i[4]=a("li",null,[a("p",null,[a("strong",null,"答案生成")]),a("p",null,[s("使用Softmax 分类器："),a("br"),s(" 将拼接后的联合特征输入到一个 softmax 层，该层作为多类别分类器，计算每个预定义答案类别的概率分布。最终，选择具有最高概率的答案作为模型的输出。")])],-1))]),i[6]||(i[6]=e(`<figure><img src="https://pica.zhimg.com/v2-4452fdaaa04686aa270010f57f4db2aa_1440w.jpg" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>softmax最终得到一个概率分布的张量，对应的概率代表答案是对应的词的概率</p><blockquote><p>Softmax <strong>只能从一个固定的候选集合中选答案</strong>。这个集合通常是 <strong>训练数据中学习到的可能答案集合</strong>，也就是 <strong>预定义的词表（Vocabulary）</strong>。也就是说无法生成自由的回答。</p></blockquote><p>github：<a href="https://github.com/zhoubolei/VQAbaseline" target="_blank" rel="noopener noreferrer">zhoubolei/VQAbaseline: Simple Baseline for Visual Question Answering</a>(但是不是用python写的，而是lua，在线的demo也已经停止运行)</p><hr><p><strong>训练的细节和结果</strong></p><p>论文中使用到的数据集是<code>COCO数据集</code>，论文中提到训练的细节：<code>在单个 NVIDIA Titan Black GPU 上训练大约需要 10 小时</code></p><blockquote><p>原文：The training takes about 10 hours on a single GPU NVIDIA Titan Black</p></blockquote><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322203830571.png" alt="image-20250322203830571" tabindex="0" loading="lazy"><figcaption>image-20250322203830571</figcaption></figure><p>这个是阿里云上GPU的价格。</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322204019083.png" alt="image-20250322204019083" tabindex="0" loading="lazy"><figcaption>image-20250322204019083</figcaption></figure><p>也就是说，按照这篇论文所说，训练这个模型，可能花费30r不到。</p><p>下面是论文中提到的测试的结果：</p><div style="display:flex;"><img width="50%" src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322205621368.png"><img width="50%" style="height:100%;" src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322205703229.png"></div> --- <p>老师注： 数据重要性大于算力，显存要求大于cuda核，训练满一点没关系。</p><p>论文里对模型的分析，要和其他的模型进行对比，还要进行<strong>剖分实验</strong> （without这个模块，之后性能怎么样？ 把这个component换成别的性能怎么样），敏感度，模型性能随着参数的变化的变化趋势</p><p>前面提到的那个baseline模型使用简单的<code>特征提取-&gt; 特征融合 -&gt; softmax得到答案</code>。</p><p>具体的来说，iBowing模型在文字编码时采用的是<strong>Bow</strong>(词袋模型). 比词袋模型用的更广泛的是：<strong>RNN</strong>(循环神经网络)。下面展示是一种早期常见的vqa方法：</p><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" data-title="mermaid" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">graph TD</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A[输入图像] --&gt;|CNN 提取特征| B[图像特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A1[输入问题] --&gt;|词嵌入 Word2Vec/GloVe| A2[词向量]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A2 --&gt;|RNN/LSTM 处理| A3[文本特征]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    B --&gt; C[拼接 Concatenation]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A3 --&gt; C</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    C --&gt;|全连接层 MLP 处理| D[融合特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    D --&gt;|Softmax 分类| E[答案预测]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>许多vqa模型都是基于transformer实现的。下图是一个简单的结构，BLIP采取的结构和这个很相似：</p><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" data-title="mermaid" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">graph TD</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  A[输入图像] --&gt;|ViT/CNN 提取特征| B[图像特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  C[输入问题] --&gt;|BERT/Tokenizer| D[文本特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  B --&gt;|跨模态融合| E[多模态 Transformer]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  D --&gt;|跨模态融合| E</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  E --&gt;|Transformer Decoder| F[生成答案]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr>`,23))])}const b=t(c,[["render",d],["__file","组会汇报week2.html.vue"]]),u=JSON.parse(`{"path":"/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/Reports/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5week2.html","title":"例会汇报 | 第二次","lang":"zh-CN","frontmatter":{"description":"例会汇报 | 第二次 （接上一次BLIP系列模型的讨论）我没有仔细看对应的论文，下方图片来自知乎。 6fb85be2a08f15ce71245f1301d3b1ab6fb85be2a08f15ce71245f1301d3b1ab 一个VQA领域的baseline模型 Baseline模型的含义： 容易实现、功能基础的模型，作为'基线'(baseline...","head":[["meta",{"property":"og:url","content":"https://github.com/yama-lei/yama-lei.github.io/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/Reports/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5week2.html"}],["meta",{"property":"og:site_name","content":"Myblog"}],["meta",{"property":"og:title","content":"例会汇报 | 第二次"}],["meta",{"property":"og:description","content":"例会汇报 | 第二次 （接上一次BLIP系列模型的讨论）我没有仔细看对应的论文，下方图片来自知乎。 6fb85be2a08f15ce71245f1301d3b1ab6fb85be2a08f15ce71245f1301d3b1ab 一个VQA领域的baseline模型 Baseline模型的含义： 容易实现、功能基础的模型，作为'基线'(baseline..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/6fb85be2a08f15ce71245f1301d3b1ab.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-04-07T06:36:45.000Z"}],["meta",{"property":"article:modified_time","content":"2025-04-07T06:36:45.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"例会汇报 | 第二次\\",\\"image\\":[\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/6fb85be2a08f15ce71245f1301d3b1ab.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/x1.png\\",\\"https://pica.zhimg.com/v2-4452fdaaa04686aa270010f57f4db2aa_1440w.jpg\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322203830571.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322204019083.png\\"],\\"dateModified\\":\\"2025-04-07T06:36:45.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Yama-lei\\",\\"url\\":\\"/underbuilding.html\\"}]}"]]},"headers":[],"git":{"createdTime":1743769747000,"updatedTime":1744007805000,"contributors":[{"name":"yama-lei","username":"yama-lei","email":"1908777046@qq.com","commits":2,"url":"https://github.com/yama-lei"}]},"readingTime":{"minutes":4.44,"words":1333},"filePathRelative":"posts/科研启蒙/Reports/组会汇报week2.md","localizedDate":"2025年4月4日","excerpt":"\\n<p>（接上一次BLIP系列模型的讨论）我没有仔细看对应的论文，下方图片来自知乎。</p>\\n<figure><img src=\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/6fb85be2a08f15ce71245f1301d3b1ab.png\\" alt=\\"6fb85be2a08f15ce71245f1301d3b1ab\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>6fb85be2a08f15ce71245f1301d3b1ab</figcaption></figure>\\n<hr>\\n<h4>一个VQA领域的baseline模型</h4>","autoDesc":true}`);export{b as comp,u as data};
