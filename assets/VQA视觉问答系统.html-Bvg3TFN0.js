import{_ as s,c as o,d as a,b as i,e as r,g as l,f as t,r as d,o as p}from"./app-CihjbUnI.js";const g="/assets/image-20250222220048981-s6BiDDDb.png",c="/assets/image-20250222220150232-DikqilHw.png",h="/assets/image-20250223085514869-DkGfkt8P.png",u="/assets/image-20250223090140313-BNLcl2ZU.png",m="/assets/image-20250223092722130-4QpZfG2a.png",b={};function f(k,e){const n=d("center");return p(),o("div",null,[e[5]||(e[5]=a('<h1 id="vqa-综述阅读" tabindex="-1"><a class="header-anchor" href="#vqa-综述阅读"><span>VQA 综述阅读：</span></a></h1><blockquote><p>This passage is a reading note of a survey on VQA. Reading the raw passage is recommened:<a href="https://arxiv.org/pdf/2411.11150" target="_blank" rel="noopener noreferrer">A Comprehensive Survey on Visual Question Answering Datasets and Algorithms</a></p></blockquote><h2 id="abstract" tabindex="-1"><a class="header-anchor" href="#abstract"><span>Abstract:</span></a></h2><p><strong>Datasets</strong>: We can devide the datasets of VQA into 4 catecories, namely:</p><p>•Available datasets that contain a rich collection of authentic images<br> •Synthetic datasets that contain only synthetic images produced through artificial means<br> •Diagnostic datasets that are specially designed to test model performance in a particular area, e.g., understanding the scene text<br> •KB (Knowledge-Based) datasets that are designed to measure a model’s ability to utilize outside knowledge</p><p><strong>Main paradigms</strong>: In this survey, we wlii explore six main paradigms:</p><ul><li>Fusion is where we discuss different methods of fusing information between visual and textual modalities.</li><li>Attention is the technique of using information from one modality to filter information from another. External knowledge base where we discuss different models utilizing outside information.</li><li>Composition or Reasoning, where we analyze techniques to answer advanced questions that require complex reasoning steps.</li><li>Explanation, which is the process of generating visual and/or textual descriptions to verify<br> sound Reasoning.</li><li>Graph models which encode and manipulate relationships through nodes in a graph.</li></ul><p>We also discuss some miscellaneous topics, such as scene text understanding, counting, and bias reduction.</p><p><strong>Problems</strong>: VQA compasses the following questions:</p><p>• Object recognition: What is behind the chair?<br> • Object detection: Are there any people in the image?<br> • Counting: How many dogs are there?<br> • Scene classification: Is it raining?<br> • Attribute classification: Is the person happy?</p><h2 id="datasets" tabindex="-1"><a class="header-anchor" href="#datasets"><span>Datasets</span></a></h2><h3 id="general-datasets" tabindex="-1"><a class="header-anchor" href="#general-datasets"><span>General datasets</span></a></h3><p>General datasets are the largest, richest, and most used datasets in VQA. General datasets contain many thousands of<br> real-world images from mainstream image datasets like MSCOCO [74] and Imagenet [32]. These datasets are notable for<br> their large scope and diversity. This variety is important as VQA datasets need to reflect the general nature of VQA. Although<br> these datasets do not necessarily capture the endless complexity and variety of visuals in real life, they achieve a close approximation.</p><figure><img src="'+g+'" alt="image-20250222220048981" tabindex="0" loading="lazy"><figcaption>image-20250222220048981</figcaption></figure><h3 id="synthetic-datasets-虚构的" tabindex="-1"><a class="header-anchor" href="#synthetic-datasets-虚构的"><span>Synthetic datasets (虚构的)</span></a></h3><p>Synthetic datasets contain artificial images, produced using software, instead of real images. A good VQA model should<br> be able to perform well on both real and synthetic data like humans do. Synthetic datasets are easier, less expensive, and<br> less time-consuming to produce as the building of a large dataset can be automated. Synthetic datasets can be tailored<br> so that performing well on them requires better reasoning and composition skills.</p><figure><img src="'+c+'" alt="image-20250222220150232" tabindex="0" loading="lazy"><figcaption>image-20250222220150232</figcaption></figure><h3 id="dignostic-datasets" tabindex="-1"><a class="header-anchor" href="#dignostic-datasets"><span>Dignostic datasets</span></a></h3><p>Diagnostic datasets are specialized in the sense that they test a model’s ability in a particular area. They are usually small in size and are meant to complement larger, more general datasets by diagnosing the model’s performance in a distinct area which may not have pronounced results in the more general dataset.</p><figure><img src="'+h+'" alt="image-20250223085514869" tabindex="0" loading="lazy"><figcaption>image-20250223085514869</figcaption></figure><h3 id="kb-datasets" tabindex="-1"><a class="header-anchor" href="#kb-datasets"><span>KB datasets</span></a></h3><p>Sometimes it is not possible to answer a question with only the information present in the image. In such cases, the required knowledge has to be acquired from external sources. This is where KB datasets come in. They provide questions that require finding and using external knowledge. KB datasets can teach a model to know when it needs to search for absent knowledge and how to acquire that knowledge.</p><figure><img src="'+u+'" alt="image-20250223090140313" tabindex="0" loading="lazy"><figcaption>image-20250223090140313</figcaption></figure><h3 id="evaluation-datasets" tabindex="-1"><a class="header-anchor" href="#evaluation-datasets"><span>Evaluation datasets</span></a></h3><p>A model’s performance being correctly evaluated depends on the evaluation metric used. Unfortunately, a major problem of VQA is that there is no widely agreed upon evaluation metric. Many different metrics have been proposed.</p><h2 id="algoritms" tabindex="-1"><a class="header-anchor" href="#algoritms"><span>Algoritms</span></a></h2><h3 id="image-representation" tabindex="-1"><a class="header-anchor" href="#image-representation"><span>Image Representation</span></a></h3><p><strong>1. CNN</strong></p><ul><li>When given an input image, a CNN goes through several convolution and pooling layers to produce a C × W × H shaped output.</li><li>Devide the image into grids</li><li>Problem: be distracted by noise (could be solved by Attention mechanism); one boject could be devided into multi adjacent blocks.</li></ul><p><strong>2. Object Detection</strong></p><ul><li><p>Example: Fast R-CNN</p></li><li><p>They produce multiple bounding boxes. Each bounding box usually contains an object belonging to a specific object class.</p></li><li><p>Devide the image into multiple &#39;bounding box&#39;</p></li><li><p>Problem: possible information loss (some information that is not in the bounding boxes would be dismissed)</p></li></ul><figure><img src="'+m+`" alt="image-20250223092722130" tabindex="0" loading="lazy"><figcaption>image-20250223092722130</figcaption></figure><p>​ <strong>CNN(left) and Faster R-CNN(right).</strong></p><h3 id="questions-representation" tabindex="-1"><a class="header-anchor" href="#questions-representation"><span>Questions Representation</span></a></h3><p>Question representation in VQA is usually done by first embedding individual words and then using an RNN or a CNN to produce an embedding of the entire question.</p><blockquote><p>Here are the explanations from Grok3:</p><ol><li><p>Take a question like &quot;What color is the car?&quot;</p></li><li><p><strong>Embed individual words</strong>: Convert each word into a vector using a word embedding technique (e.g., &quot;What&quot; → [0.1, 0.3, ...], &quot;color&quot; → [0.4, -0.1, ...], etc.).</p></li><li><p><strong>Process with an RNN or CNN</strong>:</p><ul><li><p>RNN: Feed the vectors in sequence, and the final hidden state is the question embedding.</p></li><li><p>CNN: Apply filters to the sequence, pool the results, and get the question embedding.</p></li></ul></li><li><p>The output is a single vector representing the whole question, which the VQA model can then combine with image features to generate an answer.</p></li></ol></blockquote><h3 id="fusion-and-attention" tabindex="-1"><a class="header-anchor" href="#fusion-and-attention"><span>Fusion and Attention</span></a></h3><p>We will tlk about it in the following part</p><h3 id="answering" tabindex="-1"><a class="header-anchor" href="#answering"><span>Answering</span></a></h3><p>Here’s a quick summary of how &quot;answering&quot; works in Visual Question Answering (VQA) based on the &quot;Answering&quot; section (D) from the survey’s algorithm part:</p><p>In VQA, answering can be <strong>open-ended</strong> (free-form answers) or <strong>multiple-choice</strong> (choosing from options). There are two main ways to predict answers for open-ended VQA:</p><ol><li><p><strong>Non-Generative Approach</strong> (Most Common):</p><ul><li>Treats answers as predefined classes (e.g., all unique answers in the dataset).</li><li>Two types: <ul><li><strong>Single-Label Classification</strong>: The model predicts one answer by outputting a probability distribution (using softmax) over all possible answers, trained to maximize the probability of the most agreed-upon answer from annotators. It’s simple but ignores multiple valid answers.</li><li><strong>Multi-Label Regression</strong>: The model predicts scores for multiple candidate answers, reflecting how many annotators agreed (e.g., VQA-v1 uses a soft score like <code>min(# humans agreeing / 3, 1)</code>). This handles multiple correct answers better. The BUTD model pioneered this by treating it as a regression task, and most modern models follow this approach.</li></ul></li><li><strong>Pros</strong>: Easy to implement and evaluate.</li><li><strong>Cons</strong>: Can’t predict new answers not seen in training.</li></ul></li><li><p><strong>Generative Approach</strong>:</p><ul><li>Uses an RNN to generate answers word by word.</li><li><strong>Issue</strong>: Hard to evaluate, so it’s rarely used.</li></ul></li></ol><p>For <strong>Multiple-Choice VQA</strong>:</p><ul><li>Treated as a ranking problem: The model scores each question-image-answer trio, and the highest-scoring answer wins.</li></ul><p><strong>Answer Representation</strong>:</p><ul><li>Most models use <strong>one-hot vectors</strong> (e.g., [1, 0, 0] for &quot;dog&quot;) for answers, which is simple but loses semantic meaning—e.g., &quot;cat&quot; and &quot;German Shepherd&quot; are equally wrong compared to &quot;dog.&quot;</li><li>Some newer approaches embed answers into the same semantic space as questions (like word vectors), turning answering into a regression of answer vectors. This makes &quot;German Shepherd&quot; closer to &quot;dog&quot; than &quot;cat,&quot; improving the model’s understanding and training signal.</li></ul><p>In short, modern VQA answering leans toward multi-label regression for open-ended questions, using soft scores from annotators, while multiple-choice uses ranking. Efforts are ongoing to make answer representations more semantically rich!</p><h3 id="mutilmodel-fusion" tabindex="-1"><a class="header-anchor" href="#mutilmodel-fusion"><span>Mutilmodel Fusion</span></a></h3><p>In order to perform joint reasoning on a QA pair, information from the two modalities have to mix and interact. This can be achieved by multimodal fusion. We divide fusion in VQA into two types, <strong>vector operation</strong> and <strong>bilinear pooling</strong>.</p><h4 id="vector-operation" tabindex="-1"><a class="header-anchor" href="#vector-operation"><span>Vector operation</span></a></h4><p>In vector addition and multiplication, question and image features are projected linearly through fully-connected layers to match their dimensions.</p><blockquote><p>Namely: fusion the vector of image and question by vector operation</p></blockquote><ul><li>cons: Bad Accuarcy</li></ul><h4 id="bilinear-pooling" tabindex="-1"><a class="header-anchor" href="#bilinear-pooling"><span>Bilinear pooling</span></a></h4><blockquote><p>The following content is generated by Grok3 for the raw survey is too hard for me. ಥ_ಥ</p></blockquote><p><strong>Bilinear Pooling</strong> combines question and image feature vectors (e.g., both 2048-dimensional) by computing their <strong>outer product</strong>, capturing all interactions between them. For an output ( &lt;z_i ) (answer score), it’s defined as ( z_i = x^T W_i y ), where ( x ) is the question vector, ( y ) is the image vector, and ( W ) is a huge weight tensor. However, with 3000 answer classes, this requires billions of parameters (e.g., 12.5 billion), making it computationally expensive and prone to overfitting. Different models tweak this to balance complexity and performance:</p><blockquote><ol><li><p><strong>MCB (Multimodal Compact Bilinear)</strong>:</p><ul><li>Uses a trick from math: the outer product’s &quot;count sketch&quot; can be computed as a convolution of individual sketches.</li><li>Replaces convolution with an efficient element-wise product in FFT space to indirectly get the outer product.</li><li>Still has many parameters due to fixed random settings.</li></ul></li><li><p><strong>MLB (Multimodal Low-rank Bilinear)</strong>:</p><ul><li>Reduces parameters by decomposing ( W = U V^T ), turning ( z_i = 1^T (U_i^T x \\circ V_i^T y) ) (where ( \\circ ) is element-wise multiplication).</li><li>Limits ( W )’s rank to ( k ), cutting complexity, and adds a matrix ( P_i ) for further reduction.</li><li>Downside: Slow to train and sensitive to tuning.</li></ul></li><li><p><strong>MFB (Multimodal Factorized Bilinear)</strong>:</p><ul><li>Tweaks MLB by adjusting ( U ) and ( V ) dimensions and adding <strong>sum pooling</strong> over windows of size ( k ): ( z = SumPool(U&#39;^T x \\circ V&#39;^T y, k) ).</li><li>MLB is a special case when ( k = 1 ). <strong>MFH</strong> stacks MFBs for richer pooling.</li></ul></li><li><p><strong>MUTAN (Multimodal Tucker Fusion)</strong>:</p><ul><li>Uses <strong>Tucker decomposition</strong>: ( W = \\tau_c \\times W_q \\times W_v \\times W_o ).</li><li>( W_q ) and ( W_v ) project question and image vectors, ( \\tau_c ) controls interaction complexity, and ( W_o ) scores answers.</li><li>MCB and MLB are simpler versions of this.</li></ul></li><li><p><strong>BLOCK</strong>:</p><ul><li>Uses <strong>block-term decomposition</strong>, balancing MLB (many small blocks, high-dimensional but weak interactions) and MUTAN (one big block, strong interactions but less accurate projections).</li><li>Strikes a middle ground and often performs better.</li></ul></li></ol></blockquote><p>In short, bilinear pooling fuses question and image data via their outer product, but raw computation is impractical. These models (MCB, MLB, MFB, MUTAN, BLOCK) reduce parameters in clever ways, trading off expressiveness (how much they capture) and trainability (how easy they are to optimize). Each improves on the last, with BLOCK aiming for the best of both worlds!</p><h3 id="attention" tabindex="-1"><a class="header-anchor" href="#attention"><span>Attention</span></a></h3><p>Make the model to focus on the object that are more relavant to <strong>questions</strong> to filter out noise and imrove accuarcy.</p><h4 id="soft-and-hard-attention" tabindex="-1"><a class="header-anchor" href="#soft-and-hard-attention"><span>Soft and hard attention</span></a></h4><p>Both of <strong>soft attention</strong> and <strong>hard attention</strong> use the question to make a map, which assigns the objects on the picture to different values--the more relavent, the higher the value is.</p><p>But the difference lies in:</p><ul><li><strong>Soft attention</strong> assigns all the object to a cretain value, do not dismiss any objects;</li><li><strong>Hard attention</strong> discard those with low relavance, and only cares about those relavent to the questions</li></ul><h4 id="grid-and-objct-based-attention" tabindex="-1"><a class="header-anchor" href="#grid-and-objct-based-attention"><span>Grid and objct based attention</span></a></h4><h4 id="bottom-up-and-top-down-attention" tabindex="-1"><a class="header-anchor" href="#bottom-up-and-top-down-attention"><span>BOTTOM-UP AND TOP-DOWN ATTENTION</span></a></h4><h4 id="co-attention-and-self-attention" tabindex="-1"><a class="header-anchor" href="#co-attention-and-self-attention"><span>CO-ATTENTION AND SELF-ATTENTION</span></a></h4><blockquote><p>To get more about these attention machenism, read the raw paper.</p></blockquote><h3 id="external-knowledge" tabindex="-1"><a class="header-anchor" href="#external-knowledge"><span>External Knowledge</span></a></h3><p>Sometimes the model need more information to solve the problem,int that case, we need to give the model the capability to query an <strong>External Knowledge Base</strong> or <strong>EKB</strong>.</p><hr><h1 id="例会汇报-第一次" tabindex="-1"><a class="header-anchor" href="#例会汇报-第一次"><span>例会汇报 | 第一次</span></a></h1><h2 id="vqa算法" tabindex="-1"><a class="header-anchor" href="#vqa算法"><span><strong>VQA算法</strong></span></a></h2><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" data-title="mermaid" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">graph LR</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">	图像,问题展示  --&gt; 将文字,图像特征综合 --&gt; 回答生成</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ol><li><strong>图像/问题表征方法</strong></li></ol><ul><li>图像展示 | Image representation</li><li>问题呈现 | Question representation</li></ul><ol start="2"><li><strong>多模态融合与回答生成算法</strong></li></ol><ul><li>将视觉信息和文字信息综合 | Fusion and/or Attention</li></ul><blockquote><p>The interaction of the visual and textual domain in VQA is either done directly through multimodal fusion or indirectly through attention mechanisms. --- <em>A Comprehensive Survey on Visual Question Answering Datasets and Algorithms</em></p></blockquote><ul><li>问题生成 | Answering</li></ul><h3 id="多模态融合" tabindex="-1"><a class="header-anchor" href="#多模态融合"><span>多模态融合</span></a></h3><p>“将不同模态的信息，形成一个联合的表示”</p><blockquote><p>We divide fusion in VQA into two types, vector operation and bilinear pooling.</p><pre><code>--- *A Comprehensive Survey on Visual Question Answering Datasets and Algorithms*
</code></pre></blockquote><h5 id="基于向量操作的融合" tabindex="-1"><a class="header-anchor" href="#基于向量操作的融合"><span>基于向量操作的融合</span></a></h5><ol><li>向量操作 | Vector Operation</li></ol><p>​ 通过向量操作（加法，内积，拼接），将图像特征和问题特征结合起来，生成一个联合的多模态表示。</p><ul><li>容易实现</li><li>准确度低</li></ul><ol start="2"><li>双线性池化层 | Bilinear pooling</li></ol><p>​ 通过将代表 视觉信息 和 文字信息 的 <strong>向量做外积</strong>，“比简单的向量操作（如加法、乘法或拼接）更有效地捕捉模态间的相关性。”</p><hr><h5 id="注意力机制-attention" tabindex="-1"><a class="header-anchor" href="#注意力机制-attention"><span>注意力机制 | Attention</span></a></h5><p>用于让模型<strong>聚焦</strong>输入数据中的重要部分，减少噪音的干扰</p><ul><li>注意力机制可以帮助模型动态地选择图像和文本中的重要区域或词汇</li></ul><p>注意力机制有很多种分类方式：</p><p>比如：Soft and hard attention</p><p>按照与问题的相关程度，给图像中的对象赋值。</p><p>区别在于<strong>soft attention</strong>机制不会将相关度低的对象给忽视，而<strong>hard attention</strong>则会舍弃相关度低的对象</p><p>其他的注意力机制有</p><ul><li><p>Grid and objct based attention</p></li><li><p>bottom-up and top-down attention</p></li><li><p>single setp and s multi-step attention</p></li><li><p>CO-ATTENTION AND SELF-ATTENTION</p></li></ul><hr><p>上面的内容相当于是让模型“理解”了问题和图像，下面需要生成回答：</p><h3 id="回答生成算法" tabindex="-1"><a class="header-anchor" href="#回答生成算法"><span>回答生成算法</span></a></h3><ul><li><p><strong>分类问题 | close ending</strong> ：</p><p>将前面多模态融合得到的特征输入到<code>全连接层</code>，最后通过<code>softmax</code>函数得到答案的概率分布；</p><p>(我觉得和手写数字识别类似)</p><p>​</p></li><li><p><strong>自由生成 | open ending</strong>：</p><p>生成自由的文本（和平时的大语言模型交互所生成的回答一样）;</p><p><code>编码器-阶码器</code>结构：</p><ul><li>编码器提取图像和问题的联合表示</li><li>解码器按照编码器输出，逐词生成答案</li></ul></li></ul><p>在回答生成算法中，Transformer模型比较流行:</p><p><strong>Transformer模型</strong></p><ul><li><p><strong>提取文本、图像信息，并融合</strong>：</p><ul><li>Two-Stream 图像和文本分别通过独立的 Transformer 编码器处理，最后再将两个输出融合。例子：ViLBERT, LXMERT, and ERNIE-ViL</li></ul><blockquote><p>In the two-stream architecture, two seperate transformers are applied to image and text and their outputs are fused by a third Transformer in a later stage.</p></blockquote><ul><li>Single-Stream: 将图像和文本视为一个统一的序列，通过同一个 Transformer 编码器处理。例子：ViLT、OFA、M6、VisualBERT</li></ul><blockquote><p>In contrast, single-stream models use a single transformer for joint intra-modal and inter-modal interaction.</p></blockquote></li><li><p><strong>回答生成</strong>:</p><ul><li><p><strong>分类问题 | close ending</strong>：</p><p>在 Transformer 编码器的输出后添加分类头（Classification Head）。</p><p>通过全连接层将融合后的特征映射到答案词汇表的分布。</p></li><li><p><strong>自由生成 | open ending</strong>：</p><p>使用 Transformer 的 <strong>编码器-解码器架构</strong> ，编码器得到视觉和文字特征的特征序列，输入到解码器生成答案。</p></li></ul></li></ul><hr><p>综述中提到的其他相关内容：</p><h5 id="外部知识-external-knowledge" tabindex="-1"><a class="header-anchor" href="#外部知识-external-knowledge"><span>外部知识 | EXTERNAL KNOWLEDGE</span></a></h5><p>外部知识是指从预定义的知识库（如知识图谱、数据库）或预训练模型中引入的额外知识</p><ul><li>帮助模型更好地回答问题</li></ul><h5 id="组合式推理-compositional-reasoning" tabindex="-1"><a class="header-anchor" href="#组合式推理-compositional-reasoning"><span>组合式推理 | Compositional reasoning</span></a></h5><p>将问题拆分成多个子问题，来正确地推理复杂问题</p><blockquote><p>By composition, we refer to the ability to break a question down into individual reasoning steps which when done sequentially produces the correct answer. “组合&quot;指的是将问题拆分成子问题的能力</p></blockquote><h3 id="尝试" tabindex="-1"><a class="header-anchor" href="#尝试"><span>尝试</span></a></h3><p>在了解相关模型的时候，我了解到一个基于transformer的模型框架BLIP(2022年提出)。</p><p>BLIP训练难(数据，算力)</p><ul><li><p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/blip#blip" target="_blank" rel="noopener noreferrer">HuggingFace文档-链接</a></p></li><li><p><a href="https://github.com/salesforce/BLIP" target="_blank" rel="noopener noreferrer">训练代码Giuhub-链接</a></p></li></ul><hr><p>hugging face上面有几个基于这个框架的模型，我下载了几个想看看效果是什么样的，写了一个对话的本地网页：</p><div style="display:grid;grid-template-columns:1fr 1fr;"><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250313172815938.png"><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250313173022433.png"></div> &gt; ~~**我们到时候是不是也可以将最终的模型做一个网页的demo**~~ <h3 id="有关vqa学习的疑惑" tabindex="-1"><a class="header-anchor" href="#有关vqa学习的疑惑"><span>有关VQA学习的疑惑</span></a></h3><ol><li>学习时间有限，除非放假很难有<strong>足够多</strong>的时间来系统学习，学习进度缓慢。</li><li>从哪里开始下手？需要先系统学习pytorch，transformer等吗？</li></ol><blockquote><p>python-&gt;pytorch-&gt;看一些相关实现和算法</p></blockquote><hr><h1 id="例会汇报-第二次" tabindex="-1"><a class="header-anchor" href="#例会汇报-第二次"><span>例会汇报 | 第二次</span></a></h1><p>（接上一次BLIP系列模型的讨论）我没有仔细看对应的论文，下方图片来自知乎。</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/6fb85be2a08f15ce71245f1301d3b1ab.png" alt="6fb85be2a08f15ce71245f1301d3b1ab" tabindex="0" loading="lazy"><figcaption>6fb85be2a08f15ce71245f1301d3b1ab</figcaption></figure><hr><h4 id="一个vqa领域的baseline模型" tabindex="-1"><a class="header-anchor" href="#一个vqa领域的baseline模型"><span>一个VQA领域的baseline模型</span></a></h4><blockquote><p>Baseline模型的含义： 容易实现、功能基础的模型，作为&#39;基线&#39;(baseline)。</p></blockquote><p>我找到一篇10年前的论文：<a href="paper.html">Simple Baseline for Visual Question Answering</a>，文章中提到的“iBowing”模型的结构是：</p><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" data-title="mermaid" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">graph TD</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A[输入问题（文本）] --&gt;|One-hot Encoding + 词袋模型（BoW）| B[文本特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    C[输入图像] --&gt;|CNN（如GoogLeNet）| D[图像特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    B --&gt;|拼接| E[联合特征向量]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    D --&gt;|拼接| E</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    E --&gt;|Softmax 分类| F[预测答案]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>模型核心思路如下：</p>`,135)),i("ol",null,[e[3]||(e[3]=a("<li><p><strong>文字处理</strong></p><ul><li><strong>One-hot Encoding 与 Bag-of-Words (BoW)：</strong><br> 模型首先将输入的问句转换为 one-hot 向量。在 one-hot encoding 中，每个单词都表示为一个二值向量（只有对应单词的位置为 1，其余位置为 0）。这种方法正是实现了所谓的“词袋模型”（Bag-of-Words），该模型只关注词汇的出现频率而忽略词序。</li></ul><blockquote><p>Bow（词袋模型）是只统计单词在词汇表中的出现情况，忽略了单词的语义关系和次序关系。</p><p><strong>但是</strong>，这个模型并没有直接使用one-hot向量，而是通入一个嵌入层，实现词嵌入: The input question is first converted to a one-hot vector, which is transformed to word feature via a word embedding layer</p></blockquote><ul><li><strong>Word Embedding（词嵌入）：</strong><br> 将 one-hot 向量输入到词嵌入层中，转换为低维稠密向量。词嵌入能够捕捉单词之间的语义关系，比简单的频数统计提供了更丰富的语义信息。</li></ul></li><li><p><strong>图像处理</strong></p><ul><li>利用预训练 CNN（例如 GoogLeNet）提取图像深度特征，获得图像的高层语义信息。</li></ul></li>",2)),i("li",null,[e[1]||(e[1]=a("<p><strong>特征拼接（Concatenation）：</strong><br> 将文字特征和图像特征直接进行拼接，即将两个向量<strong>横向连接</strong>，形成一个联合特征向量。这种拼接方式能够同时包含问题的文本信息和图像的视觉信息，为后续的分类提供全面的输入特征。</p><blockquote><p>这里的<strong>拼接</strong>是这个含义：</p><ul><li><strong>文本特征向量</strong>：<code>T = [0.2, 0.5, 0.3]</code> （假设 3 维向量）</li><li><strong>图像特征向量</strong>：<code>I = [0.7, 0.1, 0.9, 0.4]</code> （假设 4 维特征）</li></ul><p>那么，拼接后的 <strong>联合特征向量</strong> <code>F</code> 就是：F=[0.2,0.5,0.3,0.7,0.1,0.9,0.4]</p></blockquote>",2)),r(n,null,{default:l(()=>e[0]||(e[0]=[t("图片来自论文：https://arxiv.org/abs/1512.02167")])),_:1}),e[2]||(e[2]=i("figure",null,[i("img",{src:"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/x1.png",alt:"Refer to caption",tabindex:"0",loading:"lazy"}),i("figcaption",null,"Refer to caption")],-1))]),e[4]||(e[4]=i("li",null,[i("p",null,[i("strong",null,"答案生成")]),i("p",null,[t("使用Softmax 分类器："),i("br"),t(" 将拼接后的联合特征输入到一个 softmax 层，该层作为多类别分类器，计算每个预定义答案类别的概率分布。最终，选择具有最高概率的答案作为模型的输出。")])],-1))]),e[6]||(e[6]=a(`<figure><img src="https://pica.zhimg.com/v2-4452fdaaa04686aa270010f57f4db2aa_1440w.jpg" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>softmax最终得到一个概率分布的张量，对应的概率代表答案是对应的词的概率</p><blockquote><p>Softmax <strong>只能从一个固定的候选集合中选答案</strong>。这个集合通常是 <strong>训练数据中学习到的可能答案集合</strong>，也就是 <strong>预定义的词表（Vocabulary）</strong>。也就是说无法生成自由的回答。</p></blockquote><p>github：<a href="https://github.com/zhoubolei/VQAbaseline" target="_blank" rel="noopener noreferrer">zhoubolei/VQAbaseline: Simple Baseline for Visual Question Answering</a>(但是不是用python写的，而是lua，在线的demo也已经停止运行)</p><hr><p><strong>训练的细节和结果</strong></p><p>论文中使用到的数据集是<code>COCO数据集</code>，论文中提到训练的细节：<code>在单个 NVIDIA Titan Black GPU 上训练大约需要 10 小时</code></p><blockquote><p>原文：The training takes about 10 hours on a single GPU NVIDIA Titan Black</p></blockquote><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322203830571.png" alt="image-20250322203830571" tabindex="0" loading="lazy"><figcaption>image-20250322203830571</figcaption></figure><p>这个是阿里云上GPU的价格。</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322204019083.png" alt="image-20250322204019083" tabindex="0" loading="lazy"><figcaption>image-20250322204019083</figcaption></figure><p>也就是说，按照这篇论文所说，训练这个模型，可能花费30r不到。</p><p>下面是论文中提到的测试的结果：</p><div style="display:flex;"><img width="50%" src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322205621368.png"><img width="50%" style="height:100%;" src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322205703229.png"></div> --- <p>老师注： 数据重要性大于算力，显存要求大于cuda核，训练满一点没关系。</p><p>论文里对模型的分析，要和其他的模型进行对比，还要进行<strong>剖分实验</strong> （without这个模块，之后性能怎么样？ 把这个component换成别的性能怎么样），敏感度，模型性能随着参数的变化的变化趋势</p><p>前面提到的那个baseline模型使用简单的<code>特征提取-&gt; 特征融合 -&gt; softmax得到答案</code>。</p><p>具体的来说，iBowing模型在文字编码时采用的是<strong>Bow</strong>(词袋模型). 比词袋模型用的更广泛的是：<strong>RNN</strong>(循环神经网络)。下面展示是一种早期常见的vqa方法：</p><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" data-title="mermaid" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">graph TD</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A[输入图像] --&gt;|CNN 提取特征| B[图像特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A1[输入问题] --&gt;|词嵌入 Word2Vec/GloVe| A2[词向量]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A2 --&gt;|RNN/LSTM 处理| A3[文本特征]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    B --&gt; C[拼接 Concatenation]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    A3 --&gt; C</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    C --&gt;|全连接层 MLP 处理| D[融合特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    D --&gt;|Softmax 分类| E[答案预测]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>许多vqa模型都是基于transformer实现的。下图是一个简单的结构，BLIP采取的结构和这个很相似：</p><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" data-title="mermaid" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">graph TD</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  A[输入图像] --&gt;|ViT/CNN 提取特征| B[图像特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  C[输入问题] --&gt;|BERT/Tokenizer| D[文本特征]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  B --&gt;|跨模态融合| E[多模态 Transformer]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  D --&gt;|跨模态融合| E</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  E --&gt;|Transformer Decoder| F[生成答案]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr>`,23))])}const y=s(b,[["render",f],["__file","VQA视觉问答系统.html.vue"]]),A=JSON.parse('{"path":"/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/VQA%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html","title":"VQA视觉问答系统学习笔记","lang":"zh-CN","frontmatter":{"title":"VQA视觉问答系统学习笔记","date":"2025-02-14T00:00:00.000Z","description":"VQA 综述阅读： This passage is a reading note of a survey on VQA. Reading the raw passage is recommened:A Comprehensive Survey on Visual Question Answering Datasets and Algorithms Ab...","head":[["meta",{"property":"og:url","content":"https://github.com/yama-lei/yama-lei.github.io/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/VQA%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html"}],["meta",{"property":"og:site_name","content":"Myblog"}],["meta",{"property":"og:title","content":"VQA视觉问答系统学习笔记"}],["meta",{"property":"og:description","content":"VQA 综述阅读： This passage is a reading note of a survey on VQA. Reading the raw passage is recommened:A Comprehensive Survey on Visual Question Answering Datasets and Algorithms Ab..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/6fb85be2a08f15ce71245f1301d3b1ab.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-04-04T12:29:07.000Z"}],["meta",{"property":"article:published_time","content":"2025-02-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-04-04T12:29:07.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"VQA视觉问答系统学习笔记\\",\\"image\\":[\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/6fb85be2a08f15ce71245f1301d3b1ab.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/x1.png\\",\\"https://pica.zhimg.com/v2-4452fdaaa04686aa270010f57f4db2aa_1440w.jpg\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322203830571.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250322204019083.png\\"],\\"datePublished\\":\\"2025-02-14T00:00:00.000Z\\",\\"dateModified\\":\\"2025-04-04T12:29:07.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Yama-lei\\",\\"url\\":\\"/underbuilding.html\\"}]}"]]},"headers":[{"level":2,"title":"Abstract:","slug":"abstract","link":"#abstract","children":[]},{"level":2,"title":"Datasets","slug":"datasets","link":"#datasets","children":[{"level":3,"title":"General datasets","slug":"general-datasets","link":"#general-datasets","children":[]},{"level":3,"title":"Synthetic datasets (虚构的)","slug":"synthetic-datasets-虚构的","link":"#synthetic-datasets-虚构的","children":[]},{"level":3,"title":"Dignostic datasets","slug":"dignostic-datasets","link":"#dignostic-datasets","children":[]},{"level":3,"title":"KB datasets","slug":"kb-datasets","link":"#kb-datasets","children":[]},{"level":3,"title":"Evaluation datasets","slug":"evaluation-datasets","link":"#evaluation-datasets","children":[]}]},{"level":2,"title":"Algoritms","slug":"algoritms","link":"#algoritms","children":[{"level":3,"title":"Image Representation","slug":"image-representation","link":"#image-representation","children":[]},{"level":3,"title":"Questions Representation","slug":"questions-representation","link":"#questions-representation","children":[]},{"level":3,"title":"Fusion and Attention","slug":"fusion-and-attention","link":"#fusion-and-attention","children":[]},{"level":3,"title":"Answering","slug":"answering","link":"#answering","children":[]},{"level":3,"title":"Mutilmodel Fusion","slug":"mutilmodel-fusion","link":"#mutilmodel-fusion","children":[]},{"level":3,"title":"Attention","slug":"attention","link":"#attention","children":[]},{"level":3,"title":"External Knowledge","slug":"external-knowledge","link":"#external-knowledge","children":[]}]},{"level":2,"title":"VQA算法","slug":"vqa算法","link":"#vqa算法","children":[{"level":3,"title":"多模态融合","slug":"多模态融合","link":"#多模态融合","children":[]},{"level":3,"title":"回答生成算法","slug":"回答生成算法","link":"#回答生成算法","children":[]},{"level":3,"title":"尝试","slug":"尝试","link":"#尝试","children":[]},{"level":3,"title":"有关VQA学习的疑惑","slug":"有关vqa学习的疑惑","link":"#有关vqa学习的疑惑","children":[]}]}],"git":{"createdTime":1739588255000,"updatedTime":1743769747000,"contributors":[{"name":"yama-lei","username":"yama-lei","email":"1908777046@qq.com","commits":8,"url":"https://github.com/yama-lei"}]},"readingTime":{"minutes":14.67,"words":4402},"filePathRelative":"posts/科研启蒙/VQA视觉问答系统.md","localizedDate":"2025年2月14日","excerpt":"\\n<blockquote>\\n<p>This passage is a reading note of a survey on VQA. Reading the raw passage is recommened:<a href=\\"https://arxiv.org/pdf/2411.11150\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">A Comprehensive Survey on Visual Question Answering Datasets and Algorithms</a></p>\\n</blockquote>\\n<h2>Abstract:</h2>","autoDesc":true}');export{y as comp,A as data};
