import{_ as t,c as n,d as a,o}from"./app-DLVh_mm6.js";const i={};function r(s,e){return o(),n("div",null,e[0]||(e[0]=[a('<p>上一次在[<a href="https://arxiv.org/abs/1704.03162" target="_blank" rel="noopener noreferrer">1704.03162] Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering</a></p><p>这篇论文中，<strong>Stacked Attention</strong>的具体实现不是很懂，这篇论文给出了更加详细地实现<a href="https://arxiv.org/pdf/1511.02274" target="_blank" rel="noopener noreferrer">1511.02274</a></p><p><strong>Reference</strong></p><p><a href="https://junzx.github.io/2019/11/22/paper-QAnswering/" target="_blank" rel="noopener noreferrer">【论文笔记】Stacked Attention Networks for Image Question Answering | Blog of YQ</a></p><p><a href="https://chatgpt.com/s/dr_681cbe52e6048191b18bbed8a36370cc" target="_blank" rel="noopener noreferrer">show ask attend and answer</a></p><hr><h2 id="什么是stacked-attention" tabindex="-1"><a class="header-anchor" href="#什么是stacked-attention"><span>什么是Stacked Attention?</span></a></h2><p><strong>Stacked Attention Network（SAN）</strong> 是一种<strong>多步推理的注意力机制</strong>，它在回答问题时，不是一次性决定关注图像的哪个区域，而是<strong>分多步迭代地逐步缩小注意区域</strong>，从而提升推理能力。</p><ul><li>在每一步，网络都会根据问题与当前图像表示<strong>生成一个注意力分布</strong>（相当于给图像的每一块都加上了注意力权重）</li><li>然后使用这个注意力加权图像区域，<strong>更新问题表示</strong>（称为 refined query vector）</li><li>将这个 refined 表示继续作为下一轮 attention 的 query，形成“堆叠”的结构</li></ul><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250510191315747.png" alt="image-20250510191315747" tabindex="0" loading="lazy"><figcaption>image-20250510191315747</figcaption></figure><h2 id="sans的构成" tabindex="-1"><a class="header-anchor" href="#sans的构成"><span>SANs的构成</span></a></h2><p>The SAN consists of three major components:</p><p>(1)图像建模: <code> the image model,</code> which uses 1a CNN to extract high level image representations, e.g. one vector for each region of the image;</p><p>(2) 问题建模: <code>the question model</code>, which uses a CNN or a LSTM to extract a semantic vector of the question and .</p><p>(3) 堆叠注意力: <code>the stacked attention model</code>, which locates, via multi-step reasoning, the image regions that are relevant to the question for answer prediction.</p><p>The following image illustrates how do these components work together.</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250509182945001.png" alt="image-20250509182945001" tabindex="0" loading="lazy"><figcaption>image-20250509182945001</figcaption></figure><h2 id="image-model" tabindex="-1"><a class="header-anchor" href="#image-model"><span>Image Model</span></a></h2><ol><li><p>对图像的处理：使用CNN来处理图像，并且取池化层<code>pooling</code>的最后一层（因为具有图像内容的高级特征）</p><blockquote><table><thead><tr><th>CNN 层级</th><th>特征层次</th><th>表达能力</th></tr></thead><tbody><tr><td>前几层（Conv1、Conv2）</td><td>低级特征</td><td>边缘、颜色、纹理等</td></tr><tr><td>中间层（Conv3、Conv4）</td><td>中级特征</td><td>局部结构、轮廓</td></tr><tr><td>后几层（Conv5、FC）</td><td>高级特征</td><td>对象类别、语义、区域含义</td></tr></tbody></table></blockquote></li><li><p>具体过程：</p><ol><li><p>将图像变形为448*448px</p></li><li><p>经过CNN处理，提取出一个512* 14* 14的向量（即，图像被一个向量表示了出来，这个图像被划分成了14*14个区域，每一个区域有512维的特征,在论文中，这个向量用$f_I$来表示，$f_I$的第i列用$f_i$ 来表示，$i\\in [0,14^2-1]$ ）</p></li><li><p>与question向量进行对齐。 通过一层<code>single layer preception单层感知机</code>（即全连接层加上激活函数）<br> $$<br> v_I = tanh(W_If_I +b_I)<br> $$<br> 这里的$v_I$就是我们处理过后，对齐的图像特征</p><blockquote><p>因为图像特征和文本特征的融合需要进行融合;</p><p>$W_I$是d*512的一个矩阵，将f_i (512维) 映射成为d维。</p></blockquote></li></ol></li></ol><h2 id="question-model" tabindex="-1"><a class="header-anchor" href="#question-model"><span>Question Model</span></a></h2><blockquote><p>这里我对于问题特征的提取不是特别了解</p></blockquote><h3 id="lstm-based-question-model" tabindex="-1"><a class="header-anchor" href="#lstm-based-question-model"><span><code>LSTM</code> based question model</span></a></h3><p>将问题作为一个词序列输入</p><p>用 LSTM（长短期记忆网络）对每个词编码，最终使用最后一个隐藏状态作为问题向量 $v_Q$</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250511092514446.png" alt="image-20250511092514446" tabindex="0" loading="lazy"><figcaption>image-20250511092514446</figcaption></figure><h3 id="cnn-based-question-model" tabindex="-1"><a class="header-anchor" href="#cnn-based-question-model"><span><code>CNN</code> based question model</span></a></h3><p>将问题作为一个词向量序列（如 GloVe）输入</p><p>使用一维卷积提取局部 n-gram 特征，然后 max pooling 得到全局语义表示</p><p>更适合短句或问题结构比较固定的情况</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250511092535914.png" alt="image-20250511092535914" tabindex="0" loading="lazy"><figcaption>image-20250511092535914</figcaption></figure><h2 id="stacked-attendtion-network" tabindex="-1"><a class="header-anchor" href="#stacked-attendtion-network"><span>Stacked Attendtion Network</span></a></h2><p>前面我们已经得到了图像特征($v_I$)和文本特征($v_Q$)的表示，并且已经将他们统一到了同一维度(维度均为d).</p><blockquote><p>注意$v_I$和$v_Q$的形状不一样，</p></blockquote><p>下面是如何通过多次注意力机制来实现<code>stacked attendtion</code></p><ol><li>将图像特征和文本特征进行融合</li><li>得到每一个区域的权重分布（反应了图像和问题的相关程度）</li></ol><p>$$<br> \\begin{align} h^A &amp;= \\tanh(W_I^A v_I + (W_Q^A v_Q + b_A)) \\tag{15} \\ p^I &amp;= \\text{softmax}(W_P h^A + b_P) \\tag{16} \\end{align}<br> $$</p><blockquote><p>这些公式里面的W都代表是参数矩阵，b都代表是参数向量；上面表了一个A代表是Attendtion，I代表Image，Q代表Question</p></blockquote><p>更详细地：</p><p>$h^A$是将原本的$v_I$和$v_Q$的特征向量都映射成了k维（即$v_I$原本是d*m维的，m是图像的patch数，d是每一个patch的特征维数，但是在这个公式中左乘了一个 k * d维的向量，将其映射成了k * m维的向量。vQ同理，原本是m维向量，变成了k维向量）</p><p>得到的$h^A$ （一个k* m 维向量）再左乘一个1*k维的参数矩阵变成1 *m维，之后进入一层softmax，得到最后一个权重向量，$P_I$ 。</p><p>$P_I\\in R^m$,长得类似(0.1, 0.2,0.7),代表了每一个patch的注意力权重。</p><p>我们用这个权重对原先的Image vector进行加权，得到：<br> $$<br> v_I^`= v_IP^I<br> $$<br> (想想一下，$v_I$是一个d*m维的向量，$P^I$是一个m *1 的向量,相乘得到一个d * 1的向量)</p><p>新的图像特征和和文本特征相加得到一个<code>refined query vector</code>。可以理解为是更加精准的融合特征表示<br> $$<br> u= v_I^`+ v_Q<br> $$<br> 此时我们已经完成了一次注意力，实际上上述过程可以进行重复操作。</p><p>使用这个注意力加权图像区域u，<strong>更新问题表示</strong>（称为 refined query vector）</p><p>将这个 refined 表示继续作为下一轮 attention 的 query，形成“堆叠”的结构</p><p>即： 将u替代最初的v_Q进行迭代，多次迭代实现堆叠注意力。</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250510191315747.png" alt="image-20250510191315747" tabindex="0" loading="lazy"><figcaption>image-20250510191315747</figcaption></figure><hr><blockquote><p>query vector 好像是一个attention机制里面的专有名词， 我没有弄明白，故在这里不解释.下面是ai的解释</p><h3 id="什么是-query-vector" tabindex="-1"><a class="header-anchor" href="#什么是-query-vector"><span>什么是 Query Vector？</span></a></h3><p>在注意力机制中：</p><ul><li><strong>Query vector</strong> 是“发起注意力请求的向量”</li><li>在 VQA 中，问题的向量 $v_Q$ 就是一个 query，它“在问图像：你哪里能回答我？”</li></ul></blockquote><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250510184754070.png" alt="image-20250510184754070" tabindex="0" loading="lazy"><figcaption>image-20250510184754070</figcaption></figure><p>最后一步就是拿着得到的query vector生成答案概率：<br> $$<br> p_{ans} =softmax(W_uu^K + b_u)<br> $$</p>',51)]))}const c=t(i,[["render",r],["__file","组会汇报week6.html.vue"]]),l=JSON.parse('{"path":"/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/Reports/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5week6.html","title":"例会汇报 | 第六次","lang":"zh-CN","frontmatter":{"date":"2025-04-26T00:00:00.000Z","title":"例会汇报 | 第六次","description":"上一次在[1704.03162] Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering 这篇论文中，Stacked Attention的具体实现不是很懂，这篇论文给出了更加详细地实现1511.02274 Reference 【论文笔记】Stacked...","head":[["meta",{"property":"og:url","content":"https://github.com/yama-lei/yama-lei.github.io/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/Reports/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5week6.html"}],["meta",{"property":"og:site_name","content":"Myblog"}],["meta",{"property":"og:title","content":"例会汇报 | 第六次"}],["meta",{"property":"og:description","content":"上一次在[1704.03162] Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering 这篇论文中，Stacked Attention的具体实现不是很懂，这篇论文给出了更加详细地实现1511.02274 Reference 【论文笔记】Stacked..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250510191315747.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-15T08:39:38.000Z"}],["meta",{"property":"article:published_time","content":"2025-04-26T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-15T08:39:38.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"例会汇报 | 第六次\\",\\"image\\":[\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250510191315747.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250509182945001.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250511092514446.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250511092535914.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250510191315747.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250510184754070.png\\"],\\"datePublished\\":\\"2025-04-26T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-15T08:39:38.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Yama-lei\\",\\"url\\":\\"/underbuilding.html\\"}]}"]]},"headers":[{"level":2,"title":"什么是Stacked Attention?","slug":"什么是stacked-attention","link":"#什么是stacked-attention","children":[]},{"level":2,"title":"SANs的构成","slug":"sans的构成","link":"#sans的构成","children":[]},{"level":2,"title":"Image Model","slug":"image-model","link":"#image-model","children":[]},{"level":2,"title":"Question Model","slug":"question-model","link":"#question-model","children":[{"level":3,"title":"LSTM based question model","slug":"lstm-based-question-model","link":"#lstm-based-question-model","children":[]},{"level":3,"title":"CNN based question model","slug":"cnn-based-question-model","link":"#cnn-based-question-model","children":[]}]},{"level":2,"title":"Stacked Attendtion Network","slug":"stacked-attendtion-network","link":"#stacked-attendtion-network","children":[]}],"git":{"createdTime":1745759144000,"updatedTime":1747298378000,"contributors":[{"name":"yama-lei","username":"yama-lei","email":"1908777046@qq.com","commits":2,"url":"https://github.com/yama-lei"}]},"readingTime":{"minutes":5,"words":1499},"filePathRelative":"posts/科研启蒙/Reports/组会汇报week6.md","localizedDate":"2025年4月26日","excerpt":"<p>上一次在[<a href=\\"https://arxiv.org/abs/1704.03162\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">1704.03162] Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering</a></p>\\n<p>这篇论文中，<strong>Stacked Attention</strong>的具体实现不是很懂，这篇论文给出了更加详细地实现<a href=\\"https://arxiv.org/pdf/1511.02274\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">1511.02274</a></p>","autoDesc":true}');export{c as comp,l as data};
