import{_ as t,c as i,d as n,o as a}from"./app-DLVh_mm6.js";const o={};function r(s,e){return a(),i("div",null,e[0]||(e[0]=[n(`<h1 id="例会汇报-第一次" tabindex="-1"><a class="header-anchor" href="#例会汇报-第一次"><span>例会汇报 | 第一次</span></a></h1><h2 id="vqa算法" tabindex="-1"><a class="header-anchor" href="#vqa算法"><span><strong>VQA算法</strong></span></a></h2><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" data-title="mermaid" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">graph LR</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">	图像,问题展示  --&gt; 将文字,图像特征综合 --&gt; 回答生成</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ol><li><strong>图像/问题表征方法</strong></li></ol><ul><li>图像展示 | Image representation</li><li>问题呈现 | Question representation</li></ul><ol start="2"><li><strong>多模态融合与回答生成算法</strong></li></ol><ul><li>将视觉信息和文字信息综合 | Fusion and/or Attention</li></ul><blockquote><p>The interaction of the visual and textual domain in VQA is either done directly through multimodal fusion or indirectly through attention mechanisms. --- <em>A Comprehensive Survey on Visual Question Answering Datasets and Algorithms</em></p></blockquote><ul><li>问题生成 | Answering</li></ul><h3 id="多模态融合" tabindex="-1"><a class="header-anchor" href="#多模态融合"><span>多模态融合</span></a></h3><p>“将不同模态的信息，形成一个联合的表示”</p><blockquote><p>We divide fusion in VQA into two types, vector operation and bilinear pooling.</p><p>--- <em>A Comprehensive Survey on Visual Question Answering Datasets and Algorithms</em></p></blockquote><h5 id="基于向量操作的融合" tabindex="-1"><a class="header-anchor" href="#基于向量操作的融合"><span>基于向量操作的融合</span></a></h5><ol><li>向量操作 | Vector Operation</li></ol><pre><code>通过向量操作（加法，内积，拼接），将图像特征和问题特征结合起来，生成一个联合的多模态表示。
</code></pre><ul><li>容易实现</li><li>准确度低</li></ul><ol start="2"><li>双线性池化层 | Bilinear pooling</li></ol><pre><code>通过将代表 视觉信息 和 文字信息 的 **向量做外积**，“比简单的向量操作（如加法、乘法或拼接）更有效地捕捉模态间的相关性。”
</code></pre><hr><h5 id="注意力机制-attention" tabindex="-1"><a class="header-anchor" href="#注意力机制-attention"><span>注意力机制 | Attention</span></a></h5><p>用于让模型<strong>聚焦</strong>输入数据中的重要部分，减少噪音的干扰</p><ul><li>注意力机制可以帮助模型动态地选择图像和文本中的重要区域或词汇</li></ul><p>注意力机制有很多种分类方式：</p><p>比如：Soft and hard attention</p><p>按照与问题的相关程度，给图像中的对象赋值。</p><p>区别在于<strong>soft attention</strong>机制不会将相关度低的对象给忽视，而<strong>hard attention</strong>则会舍弃相关度低的对象</p><p>其他的注意力机制有</p><ul><li><p>Grid and objct based attention</p></li><li><p>bottom-up and top-down attention</p></li><li><p>single setp and s multi-step attention</p></li><li><p>CO-ATTENTION AND SELF-ATTENTION</p></li></ul><hr><p>上面的内容相当于是让模型“理解”了问题和图像，下面需要生成回答：</p><h3 id="回答生成算法" tabindex="-1"><a class="header-anchor" href="#回答生成算法"><span>回答生成算法</span></a></h3><ul><li><p><strong>分类问题 | close ending</strong> ：</p><p>将前面多模态融合得到的特征输入到<code>全连接层</code>，最后通过<code>softmax</code>函数得到答案的概率分布；</p><p>(我觉得和手写数字识别类似)</p><p>​</p></li><li><p><strong>自由生成 | open ending</strong>：</p><p>生成自由的文本（和平时的大语言模型交互所生成的回答一样）;</p><p><code>编码器-阶码器</code>结构：</p><ul><li>编码器提取图像和问题的联合表示</li><li>解码器按照编码器输出，逐词生成答案</li></ul></li></ul><p>在回答生成算法中，Transformer模型比较流行:</p><p><strong>Transformer模型</strong></p><ul><li><p><strong>提取文本、图像信息，并融合</strong>：</p><ul><li>Two-Stream 图像和文本分别通过独立的 Transformer 编码器处理，最后再将两个输出融合。例子：ViLBERT, LXMERT, and ERNIE-ViL</li></ul><blockquote><p>In the two-stream architecture, two seperate transformers are applied to image and text and their outputs are fused by a third Transformer in a later stage.</p></blockquote><ul><li>Single-Stream: 将图像和文本视为一个统一的序列，通过同一个 Transformer 编码器处理。例子：ViLT、OFA、M6、VisualBERT</li></ul><blockquote><p>In contrast, single-stream models use a single transformer for joint intra-modal and inter-modal interaction.</p></blockquote></li><li><p><strong>回答生成</strong>:</p><ul><li><p><strong>分类问题 | close ending</strong>：</p><p>在 Transformer 编码器的输出后添加分类头（Classification Head）。</p><p>通过全连接层将融合后的特征映射到答案词汇表的分布。</p></li><li><p><strong>自由生成 | open ending</strong>：</p><p>使用 Transformer 的 <strong>编码器-解码器架构</strong> ，编码器得到视觉和文字特征的特征序列，输入到解码器生成答案。</p></li></ul></li></ul><hr><p>综述中提到的其他相关内容：</p><h5 id="外部知识-external-knowledge" tabindex="-1"><a class="header-anchor" href="#外部知识-external-knowledge"><span>外部知识 | EXTERNAL KNOWLEDGE</span></a></h5><p>外部知识是指从预定义的知识库（如知识图谱、数据库）或预训练模型中引入的额外知识</p><ul><li>帮助模型更好地回答问题</li></ul><h5 id="组合式推理-compositional-reasoning" tabindex="-1"><a class="header-anchor" href="#组合式推理-compositional-reasoning"><span>组合式推理 | Compositional reasoning</span></a></h5><p>将问题拆分成多个子问题，来正确地推理复杂问题</p><blockquote><p>By composition, we refer to the ability to break a question down into individual reasoning steps which when done sequentially produces the correct answer. “组合&quot;指的是将问题拆分成子问题的能力</p></blockquote><h3 id="尝试" tabindex="-1"><a class="header-anchor" href="#尝试"><span>尝试</span></a></h3><p>在了解相关模型的时候，我了解到一个基于transformer的模型框架BLIP(2022年提出)。</p><p>BLIP训练难(数据，算力)</p><ul><li><p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/blip#blip" target="_blank" rel="noopener noreferrer">HuggingFace文档-链接</a></p></li><li><p><a href="https://github.com/salesforce/BLIP" target="_blank" rel="noopener noreferrer">训练代码Giuhub-链接</a></p></li></ul><hr><p>hugging face上面有几个基于这个框架的模型，我下载了几个想看看效果是什么样的，写了一个对话的本地网页：</p><div style="display:grid;grid-template-columns:1fr 1fr;"><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250313172815938.png"><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250313173022433.png"></div> &gt; ~~**我们到时候是不是也可以将最终的模型做一个网页的demo**~~ <h3 id="有关vqa学习的疑惑" tabindex="-1"><a class="header-anchor" href="#有关vqa学习的疑惑"><span>有关VQA学习的疑惑</span></a></h3><ol><li>学习时间有限，除非放假很难有<strong>足够多</strong>的时间来系统学习，学习进度缓慢。</li><li>从哪里开始下手？需要先系统学习pytorch，transformer等吗？</li></ol><blockquote><p>python-&gt;pytorch-&gt;看一些相关实现和算法</p></blockquote><hr>`,55)]))}const p=t(o,[["render",r],["__file","组会汇报week1.html.vue"]]),d=JSON.parse('{"path":"/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/Reports/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5week1.html","title":"例会汇报 | 第一次","lang":"zh-CN","frontmatter":{"title":"例会汇报 | 第一次","description":"例会汇报 | 第一次 VQA算法 图像/问题表征方法 图像展示 | Image representation 问题呈现 | Question representation 多模态融合与回答生成算法 将视觉信息和文字信息综合 | Fusion and/or Attention The interaction of the visual and textu...","head":[["meta",{"property":"og:url","content":"https://github.com/yama-lei/yama-lei.github.io/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/Reports/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5week1.html"}],["meta",{"property":"og:site_name","content":"Myblog"}],["meta",{"property":"og:title","content":"例会汇报 | 第一次"}],["meta",{"property":"og:description","content":"例会汇报 | 第一次 VQA算法 图像/问题表征方法 图像展示 | Image representation 问题呈现 | Question representation 多模态融合与回答生成算法 将视觉信息和文字信息综合 | Fusion and/or Attention The interaction of the visual and textu..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-04-07T12:54:41.000Z"}],["meta",{"property":"article:modified_time","content":"2025-04-07T12:54:41.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"例会汇报 | 第一次\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-04-07T12:54:41.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Yama-lei\\",\\"url\\":\\"/underbuilding.html\\"}]}"]]},"headers":[{"level":2,"title":"VQA算法","slug":"vqa算法","link":"#vqa算法","children":[{"level":3,"title":"多模态融合","slug":"多模态融合","link":"#多模态融合","children":[]},{"level":3,"title":"回答生成算法","slug":"回答生成算法","link":"#回答生成算法","children":[]},{"level":3,"title":"尝试","slug":"尝试","link":"#尝试","children":[]},{"level":3,"title":"有关VQA学习的疑惑","slug":"有关vqa学习的疑惑","link":"#有关vqa学习的疑惑","children":[]}]}],"git":{"createdTime":1743769747000,"updatedTime":1744030481000,"contributors":[{"name":"yama-lei","username":"yama-lei","email":"1908777046@qq.com","commits":4,"url":"https://github.com/yama-lei"}]},"readingTime":{"minutes":4.03,"words":1208},"filePathRelative":"posts/科研启蒙/Reports/组会汇报week1.md","localizedDate":"2025年4月4日","excerpt":"\\n<h2><strong>VQA算法</strong></h2>\\n<div class=\\"language-mermaid line-numbers-mode\\" data-highlighter=\\"shiki\\" data-ext=\\"mermaid\\" data-title=\\"mermaid\\" style=\\"--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34\\"><pre class=\\"shiki shiki-themes one-light one-dark-pro vp-code\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#383A42;--shiki-dark:#ABB2BF\\">graph LR</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#383A42;--shiki-dark:#ABB2BF\\">\\t图像,问题展示  --&gt; 将文字,图像特征综合 --&gt; 回答生成</span></span></code></pre>\\n<div class=\\"line-numbers\\" aria-hidden=\\"true\\" style=\\"counter-reset:line-number 0\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","autoDesc":true}');export{p as comp,d as data};
