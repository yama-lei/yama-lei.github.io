import{_ as l,c as e,d as n,e as p,g as h,b as i,f as a,r,o as g}from"./app-DhKuMFOp.js";const o={};function k(c,s){const t=r("center");return g(),e("div",null,[s[1]||(s[1]=n('<hr><h2 id="一个成功在服务器上跑起来的模型" tabindex="-1"><a class="header-anchor" href="#一个成功在服务器上跑起来的模型"><span>一个成功在服务器上跑起来的模型</span></a></h2><p>很久以前，组会分享中介绍过一个VQA的baseline模型<a href="https://arxiv.org/pdf/1512.02167" target="_blank" rel="noopener noreferrer">arxiv-1512.02167</a>。</p><p>因为其结构简单，数据集仅使用了COCO数据集</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/x1.png" alt="Refer to caption" tabindex="0" loading="lazy"><figcaption>Refer to caption</figcaption></figure><p>但是原先项目是用 lua写的，我想试试pytorch改写。</p><p>于是<s>我在ai的帮助下</s><strong>ai在我的帮助下</strong>，尝试构造了这个模型：</p><p>过程：</p><ul><li>本地构建模型（本地可以也用一个venv环境，然后再导出一个requiremnets.txt文件即可）</li><li>服务器创建环境并运行模型</li><li>将保存好的模型下载到本地</li><li>成功运行</li></ul><p><strong>在部署中遇见的困难和踩到的坑</strong></p><ol><li><p>数据大，传输速度慢</p></li><li><p>环境搭建</p></li><li><p>数据集出问题</p></li><li><p>下载</p></li><li><p>给ai坑了。。。</p><p>训练了一个小时，发现正确率依旧是33%上下，我觉得很纳闷，回去看模型发现：</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/1745410249937.png" alt="1745410249937" tabindex="0" loading="lazy"><figcaption>1745410249937</figcaption></figure></li></ol><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250423201121073.png" alt="image-20250423201121073" tabindex="0" loading="lazy"><figcaption>image-20250423201121073</figcaption></figure><hr><p>使用真实数据之后，最后成功开始训练，20个Epoch，batchsize为320，GPU利用率平均75%；</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250423202924246.png" alt="image-20250423202924246" tabindex="0" loading="lazy"><figcaption>image-20250423202924246</figcaption></figure><p>每一个epoch大概用时3min，一共训练了20个epoch，准确率提升到了92%,总用时57min</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250423213926209.png" alt="image-20250423213926209" tabindex="0" loading="lazy"><figcaption>image-20250423213926209</figcaption></figure><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250423213913139.png" alt="Figure_1" tabindex="0" loading="lazy"><figcaption>Figure_1</figcaption></figure><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/Figure_1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>但是当我把训练好的模型拿到本地进行测试时，发现了很尴尬的一幕：</p><div style="display:flex;gap:20px;"><div style="text-align:left;"><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/20250423215715.png"></div><div style="text-align:left;"><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/20250423215741.png"></div></div><p>所有的问题，都会回答<code>the</code>。</p><p>检查发现，<strong>论文里面说的是COCO VQA数据集，我虽然使用的是COCO2017图像，但是使用的标注集都是caption</strong>，用于训练图像描述的；</p>',23)),p(t,null,{default:h(()=>s[0]||(s[0]=[i("del",null,"但凡我和ai有一个会深度学习，也不会出现这么尴尬的事情",-1)])),_:1}),s[2]||(s[2]=n(`<hr><p>之后还尝试了在COCO2017基础上的一些数据集，但是模型还是不太靠谱，最后效果十分地差。</p><p><s>这个悲伤的故事告诉我们，学习要按部就班，一步一个脚印，踏踏实实地学习，夯实基础。</s></p><p>......</p><hr><p>但是我还是不死心，在4/25再试了一次：</p><ul><li>下载了COCO2014数据集和VQA-v2对这个数据集的标注</li><li>仍然使用预训练好的CNN模型来提取图像特征，使用LSTM对问题进行编码，使用简单的拼接来特征融合</li></ul><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250425224401342.png" alt="image-20250425224401342" tabindex="0" loading="lazy"><figcaption>image-20250425224401342</figcaption></figure><p>训练了30个epoch，每一个epoch耗时约20min，batchsize设置为64。</p><p>早上起来之后发现还没训练完，一看acc和loss，觉得没有训练的必要了</p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250426084020076.png" alt="image-20250426084020076" tabindex="0" loading="lazy"><figcaption>image-20250426084020076</figcaption></figure><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250426085312953.png" alt="image-20250426085312953" tabindex="0" loading="lazy"><figcaption>image-20250426085312953</figcaption></figure><p>没错，我又浪费了国家的电。</p><hr><p>我还是不服气，最后找了一个论文的复现：</p><hr><h2 id="vqa早期论文阅读" tabindex="-1"><a class="header-anchor" href="#vqa早期论文阅读"><span>VQA早期论文阅读</span></a></h2><p><a href="https://arxiv.org/pdf/1704.03162" target="_blank" rel="noopener noreferrer">1704.03162</a></p><figure><img src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250426205954215.png" alt="image-20250426205954215" tabindex="0" loading="lazy"><figcaption>image-20250426205954215</figcaption></figure><hr><p>模型结构</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">class</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> Net</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">nn</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">Module</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">)</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    重新实现论文 \`\`Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering&#39;&#39; [0]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    这个类定义了一个用于视觉问答（VQA）的神经网络。</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    它处理视觉特征（图像）和文本特征（问题）以预测答案。</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    [0]: https://arxiv.org/abs/1704.03162</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> forward</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> v</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> q</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> q_len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        网络的前向传播。</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        参数:</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">            v (torch.Tensor): 视觉特征（图像嵌入），形状：(batch_size, vision_features, height, width)</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">            q (torch.Tensor): 问题嵌入，形状：(batch_size, max_question_length)</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">            q_len (torch.Tensor): 每个问题的实际长度，形状：(batch_size)</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        返回:</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">            torch.Tensor: 预测答案的 logits，形状：(batch_size, num_answers)</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 通过 LSTM 处理问题，得到问题特征</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        q </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;"> self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">text</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(q, </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">list</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(q_len.data)) </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 对视觉特征进行 L2 归一化</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        v </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> v </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> (v.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">norm</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">p</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">dim</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">keepdim</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">expand_as</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(v) </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1e-8</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 基于问题特征计算视觉特征的注意力权重</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        a </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;"> self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">attention</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(v, q) </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 应用注意力机制到视觉特征上，得到加权后的视觉特征</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        v </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> apply_attention</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(v, a)  </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 将加权后的视觉特征和问题特征拼接在一起</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        combined </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">cat</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">([v, q], </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">dim</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        vision_features </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> question_features)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 将组合后的特征传递给分类器，预测答案</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        answer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;"> self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">classifier</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(combined)  </span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> answer</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="图像特征提取" tabindex="-1"><a class="header-anchor" href="#图像特征提取"><span><strong>图像特征提取</strong></span></a></h3><ul><li>采用152层深度残差网络（ResNet-152）作为图像编码器，基于ImageNet预训练权重进行特征提取。</li></ul><hr><h3 id="问题特征提取" tabindex="-1"><a class="header-anchor" href="#问题特征提取"><span><strong>问题特征提取</strong></span></a></h3><ul><li>使用单向长短期记忆网络（LSTM）对问题进行编码。</li></ul><hr><h3 id="特征融合与注意力机制" tabindex="-1"><a class="header-anchor" href="#特征融合与注意力机制"><span><strong>特征融合与注意力机制</strong></span></a></h3><p>这一部分我没看懂，这是ai生成的内容:</p>`,30)),s[3]||(s[3]=i("ul",null,[i("li",null,[i("strong",null,"多模态交互"),a("：通过堆叠注意力机制（Stacked Attention）实现图像与问题的协同推理。 "),i("ul",null,[i("li",null,[i("strong",null,"注意力权重计算"),a("： "),i("ol",null,[i("li",null,[i("strong",null,"特征拼接"),a("：将问题向量$\\mathbf{s}$沿空间维度复制为$14 \\times 14 \\times 1024$，与图像特征$\\phi \\in \\mathbb{R}^{14 \\times 14 \\times 2048}$拼接，形成联合特征$\\psi \\in \\mathbb{R}^{14 \\times 14 \\times 3072}$。")]),i("li",null,[i("strong",null,"卷积映射"),a("：通过两层卷积操作生成注意力分布： "),i("ul",null,[i("li",null,"第一层：$1 \\times 1$卷积，输出通道数512，激活函数为ReLU。"),i("li",null,"第二层：$1 \\times 1$卷积，输出通道数$C=2$，对应两个独立的注意力头。")])]),i("li",null,[i("strong",null,"归一化"),a("：对每个注意力头$c$在空间维度上应用Softmax，得到归一化权重$\\alpha_{c,l} \\propto \\exp(F_c(\\psi_l))$，满足$\\sum_{l=1}^{14 \\times 14} \\alpha_{c,l} = 1$。")])])]),i("li",null,[i("strong",null,"特征聚合"),a("：每个注意力头$c$生成的特征向量为："),i("br"),a(" $$\\mathbf{x}"),i("em",{l:"1"},"c = \\sum"),a("^{14 \\times 14} \\alpha_{c,l} \\phi_l \\in \\mathbb{R}^{2048}$$"),i("br"),a(" 最终拼接两个注意力头的结果$\\mathbf{x} = [\\mathbf{x}_1, \\mathbf{x}_2] \\in \\mathbb{R}^{4096}$。")])])])],-1)),s[4]||(s[4]=n('<hr><h3 id="分类器与输出生成" tabindex="-1"><a class="header-anchor" href="#分类器与输出生成"><span><strong>分类器与输出生成</strong></span></a></h3><p>将注意力特征$\\mathbf{x}$与问题向量$\\mathbf{s}$拼接，输入分类器。</p><p><strong>网络结构</strong>：</p><ol><li><strong>全连接层</strong>：维度降至1024，激活函数为ReLU，应用Dropout（概率0.5）。</li><li><strong>输出层</strong>：线性映射至答案空间$\\mathbb{R}^{3000}$，覆盖训练集中最高频的3000个答案（覆盖率92%）。</li></ol><p><strong>概率生成</strong>：通过Softmax函数计算答案概率分布：<br> $$P(a_i | I, q) = \\frac{\\exp(G_i(\\mathbf{h}))}{\\sum_{j=1}^{3000} \\exp(G_j(\\mathbf{h}))}$$</p><hr><p>复现的仓库：<a href="https://github.com/Cyanogenoid/pytorch-vqa" target="_blank" rel="noopener noreferrer">Cyanogenoid/pytorch-vqa: Strong baseline for visual question answering</a></p><ol><li>提取图像特征</li><li>从答案中得到词汇表</li><li>开始训练</li></ol>',9))])}const m=l(o,[["render",k],["__file","组会汇报week5.html.vue"]]),y=JSON.parse('{"path":"/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/Reports/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5week5.html","title":"例会汇报 | 第五次","lang":"zh-CN","frontmatter":{"title":"例会汇报 | 第五次","date":"2023-04-23T00:00:00.000Z","description":"一个成功在服务器上跑起来的模型 很久以前，组会分享中介绍过一个VQA的baseline模型arxiv-1512.02167。 因为其结构简单，数据集仅使用了COCO数据集 Refer to captionRefer to caption 但是原先项目是用 lua写的，我想试试pytorch改写。 于是ai在我的帮助下，尝试构造了这个模型： 过程： 本地...","head":[["meta",{"property":"og:url","content":"https://github.com/yama-lei/yama-lei.github.io/posts/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/Reports/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5week5.html"}],["meta",{"property":"og:site_name","content":"Myblog"}],["meta",{"property":"og:title","content":"例会汇报 | 第五次"}],["meta",{"property":"og:description","content":"一个成功在服务器上跑起来的模型 很久以前，组会分享中介绍过一个VQA的baseline模型arxiv-1512.02167。 因为其结构简单，数据集仅使用了COCO数据集 Refer to captionRefer to caption 但是原先项目是用 lua写的，我想试试pytorch改写。 于是ai在我的帮助下，尝试构造了这个模型： 过程： 本地..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/x1.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-04-27T13:05:44.000Z"}],["meta",{"property":"article:published_time","content":"2023-04-23T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-04-27T13:05:44.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"例会汇报 | 第五次\\",\\"image\\":[\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/x1.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/1745410249937.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250423201121073.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250423202924246.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250423213926209.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250423213913139.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/Figure_1.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250425224401342.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250426084020076.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250426085312953.png\\",\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/image-20250426205954215.png\\"],\\"datePublished\\":\\"2023-04-23T00:00:00.000Z\\",\\"dateModified\\":\\"2025-04-27T13:05:44.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Yama-lei\\",\\"url\\":\\"/underbuilding.html\\"}]}"]]},"headers":[{"level":2,"title":"一个成功在服务器上跑起来的模型","slug":"一个成功在服务器上跑起来的模型","link":"#一个成功在服务器上跑起来的模型","children":[]},{"level":2,"title":"VQA早期论文阅读","slug":"vqa早期论文阅读","link":"#vqa早期论文阅读","children":[{"level":3,"title":"图像特征提取","slug":"图像特征提取","link":"#图像特征提取","children":[]},{"level":3,"title":"问题特征提取","slug":"问题特征提取","link":"#问题特征提取","children":[]},{"level":3,"title":"特征融合与注意力机制","slug":"特征融合与注意力机制","link":"#特征融合与注意力机制","children":[]},{"level":3,"title":"分类器与输出生成","slug":"分类器与输出生成","link":"#分类器与输出生成","children":[]}]}],"git":{"createdTime":1744007805000,"updatedTime":1745759144000,"contributors":[{"name":"yama-lei","username":"yama-lei","email":"1908777046@qq.com","commits":2,"url":"https://github.com/yama-lei"}]},"readingTime":{"minutes":5.16,"words":1548},"filePathRelative":"posts/科研启蒙/Reports/组会汇报week5.md","localizedDate":"2023年4月23日","excerpt":"<hr>\\n<h2>一个成功在服务器上跑起来的模型</h2>\\n<p>很久以前，组会分享中介绍过一个VQA的baseline模型<a href=\\"https://arxiv.org/pdf/1512.02167\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">arxiv-1512.02167</a>。</p>\\n<p>因为其结构简单，数据集仅使用了COCO数据集</p>\\n<figure><img src=\\"https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/x1.png\\" alt=\\"Refer to caption\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>Refer to caption</figcaption></figure>","autoDesc":true}');export{m as comp,y as data};
