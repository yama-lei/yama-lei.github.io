<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Pytorch学习 | YamaBlog</title><meta name="author" content="Yama-lei"><meta name="copyright" content="Yama-lei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="声明： 本文含有从各类教程中获取的内容和ai辅助生成内容  在正式学习之前，先介绍两个很好用的函数:dir()和help()。 一个包好比一个工具箱，工具箱下可能有其他的格子，各自下可能有工具，也可能还有格子。  dir 用于看子模块的结构 help用于看某个东西的作用  比如 1234567import torchprint(dir(torch)&#x2F;&#x2F;得到很多个东西，其中有cuda print(">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch学习">
<meta property="og:url" content="https://yama-lei.top/posts/3227e790fe57/index.html">
<meta property="og:site_name" content="YamaBlog">
<meta property="og:description" content="声明： 本文含有从各类教程中获取的内容和ai辅助生成内容  在正式学习之前，先介绍两个很好用的函数:dir()和help()。 一个包好比一个工具箱，工具箱下可能有其他的格子，各自下可能有工具，也可能还有格子。  dir 用于看子模块的结构 help用于看某个东西的作用  比如 1234567import torchprint(dir(torch)&#x2F;&#x2F;得到很多个东西，其中有cuda print(">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/yama.webp">
<meta property="article:published_time" content="2025-09-28T13:20:28.712Z">
<meta property="article:modified_time" content="2025-09-28T13:41:20.028Z">
<meta property="article:author" content="Yama-lei">
<meta property="article:tag" content="博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/yama.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Pytorch学习",
  "url": "https://yama-lei.top/posts/3227e790fe57/",
  "image": "https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/yama.webp",
  "datePublished": "2025-09-28T13:20:28.712Z",
  "dateModified": "2025-09-28T13:41:20.028Z",
  "author": [
    {
      "@type": "Person",
      "name": "Yama-lei",
      "url": "https://yama-lei.top"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yama-lei.top/posts/3227e790fe57/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch学习',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/callout_blocks.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/homepage-hero.css"><link rel="stylesheet" href="/css/card-enhance.css"><link rel="stylesheet" href="/css/sidebar-enhance.css"><link rel="stylesheet" href="/css/category-tag-page.css"><meta name="generator" content="Hexo 8.0.0"><link rel="alternate" href="/atom.xml" title="YamaBlog" type="application/atom+xml">
</head><body><div id="web_bg" style="background-color: #ffffff;"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/%E5%9B%BE%E7%89%87%E6%AD%A3%E5%9C%A8%E5%8A%A0%E8%BD%BD%E4%B8%AD..." data-lazy-src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/yama.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">105</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-list"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/essay/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/playground/"><i class="fa-fw fas fa-flask"></i><span> Playground</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-user-friends"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">YamaBlog</span></a><a class="nav-page-title" href="/"><span class="site-name">Pytorch学习</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-list"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/essay/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/playground/"><i class="fa-fw fas fa-flask"></i><span> Playground</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-user-friends"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Pytorch学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-28T13:20:28.712Z" title="发表于 2025-09-28 21:20:28">2025-09-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-28T13:41:20.028Z" title="更新于 2025-09-28 21:41:20">2025-09-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94%E5%90%AF%E8%92%99/">科研启蒙</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><blockquote>
<p>声明： 本文含有从各类教程中获取的内容和ai辅助生成内容</p>
</blockquote>
<p>在正式学习之前，先介绍两个很好用的函数:dir()和help()。</p>
<p>一个包好比一个工具箱，工具箱下可能有其他的格子，各自下可能有工具，也可能还有格子。</p>
<ul>
<li>dir 用于看子模块的结构</li>
<li>help用于看某个东西的作用</li>
</ul>
<p>比如</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch)</span><br><span class="line">//得到很多个东西，其中有cuda</span><br><span class="line"> <span class="built_in">print</span>(<span class="built_in">dir</span>(torch.cuda))</span><br><span class="line"> //得到很多歌东西，其中有is_available</span><br><span class="line">  <span class="built_in">print</span>(<span class="built_in">help</span>(torch.cuda.is_avaiable))</span><br><span class="line">  //得到有关这个的作用</span><br></pre></td></tr></table></figure>
<h2 id="张量tensor"><a class="markdownIt-Anchor" href="#张量tensor"></a> 张量：tensor</h2>
<p>张量其实可以视为一种多维数组，用于存储和操纵多维数组。</p>
<ul>
<li>维度（Dimensionality） 一个标量是0维度、的张量，一个一维数组是一个一维的张量。</li>
<li>形状（Shape）张量的形状指的是在每个维度的大小，一个3X4的张量意味着它有三行四列。</li>
<li>数据类型（Dtype）有torch.int8,torch.int32,torch.float32等等</li>
</ul>
<blockquote>
<p>Reading this passage in Zhihu is highly recommended: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/131591991">深度学习中关于张量的阶、轴和形状的解释 | Pytorch系列（二） - 知乎</a></p>
</blockquote>
<h3 id="tensor的属性"><a class="markdownIt-Anchor" href="#tensor的属性"></a> tensor的属性</h3>
<p><strong>前四个和数据相关</strong></p>
<ol>
<li>data： 被封装的tensor</li>
<li>dtype： 张量的数据类型</li>
<li>shape： 张量的形状</li>
<li>device : 获取张量所在设备，GPU/CPU</li>
</ol>
<p><strong>下面四个和梯度求导息息相关</strong></p>
<ol start="5">
<li>requires_grad: 是否需要计算梯度</li>
<li>grad: 梯度</li>
<li>grad_fn: 创建tensor的函数，是自动求导的关键所在</li>
<li>is_leaf: 指示张量是否为叶子结点</li>
</ol>
<h3 id="tensor张量的创建"><a class="markdownIt-Anchor" href="#tensor张量的创建"></a> tensor张量的创建</h3>
<h4 id="一-用torchtensor方法"><a class="markdownIt-Anchor" href="#一-用torchtensor方法"></a> <strong>一. 用torch.tensor()方法</strong></h4>
<p>将Python原生的列表或者numpy的Array对象作为参数，可以创建张量：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensorA=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment">#tensorA是一个一维张量</span></span><br><span class="line">tensorB=torch.tensor(np.zeros([<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line"><span class="comment">#tensorB是一个二维张量</span></span><br><span class="line">tensorC=torch.tensor(np.Array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line"><span class="comment">#tensorC是一个一维张量</span></span><br></pre></td></tr></table></figure>
<h4 id="二-使用内置函数创建特殊形状的函数"><a class="markdownIt-Anchor" href="#二-使用内置函数创建特殊形状的函数"></a> <strong>二. 使用内置函数创建特殊形状的函数</strong></h4>
<p>你可以使用一些内置函数来创建特定形状的张量。</p>
<ul>
<li><strong><code>torch.zeros()</code></strong> : 创建全为 0 的张量。</li>
<li><strong><code>torch.ones()</code></strong> : 创建全为 1 的张量。</li>
<li><strong><code>torch.empty()</code></strong> : 创建未初始化的张量（内容是随机的）。</li>
<li><strong><code>torch.rand()</code></strong> : 创建元素值在 [0, 1) 范围内的随机张量。</li>
<li><strong><code>torch.randn()</code></strong> : 创建服从标准<code>标准正态分布</code>的随机张量。</li>
</ul>
<blockquote>
<p>这些函数的参数都是多个整数，代表形状</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a=torch.zeros(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a: &#x27;</span>,a)</span><br><span class="line">b=torch.ones(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b: &#x27;</span>,b)</span><br><span class="line">c=torch.rand(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c: &#x27;</span>,c)</span><br><span class="line">d=torch.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;d: &#x27;</span>,d)</span><br></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a:  tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line">b:  tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">c:  tensor([[<span class="number">0.4796</span>],</span><br><span class="line">        [<span class="number">0.9827</span>],</span><br><span class="line">        [<span class="number">0.9631</span>],</span><br><span class="line">        [<span class="number">0.6458</span>]])</span><br><span class="line">d:  tensor([[-<span class="number">1.8295</span>, -<span class="number">0.7194</span>],</span><br><span class="line">        [ <span class="number">0.5028</span>,  <span class="number">0.3128</span>],</span><br><span class="line">        [-<span class="number">1.5890</span>,  <span class="number">0.1775</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如何理解高维的tensor？以这里的张量a为例，a的shape是(2,3,4),可以理解为2个shape为(3,4)的tensor 组合而成；shape为(3,4)的张量又是3个shape为(4,)的张量构成</p>
</blockquote>
<p><strong>三. 创建等差数列</strong></p>
<p>你可以使用 <code>torch.arange()</code> 或 <code>torch.linspace()</code> 来创建等差数列的张量。</p>
<ul>
<li><strong><code>torch.arange(start, end, step)</code></strong> : 创建从 <code>start</code> 到 <code>end</code>（不包括 <code>end</code>），步长为 <code>step</code> 的张量。</li>
<li><strong><code>torch.linspace(start, end, steps)</code></strong> : 创建从 <code>start</code> 到 <code>end</code>（包括 <code>end</code>），分为 <code>steps</code> 个点的张量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">2</span>,<span class="number">3</span>,<span class="number">0.1</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.linspace(<span class="number">3</span>,<span class="number">4</span>,<span class="number">10</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000, 2.6000, 2.7000, 2.8000,</span></span><br><span class="line"><span class="string">        2.9000])</span></span><br><span class="line"><span class="string">tensor([3.0000, 3.1111, 3.2222, 3.3333, 3.4444, 3.5556, 3.6667, 3.7778, 3.8889,</span></span><br><span class="line"><span class="string">        4.0000])</span></span><br></pre></td></tr></table></figure>
<p><strong>注意torch.linspace的第三个参数是’份数‘，而torch.arange的第三个参数是步长</strong></p>
<h4 id="四-创建单位矩阵"><a class="markdownIt-Anchor" href="#四-创建单位矩阵"></a> <strong>四. 创建单位矩阵</strong></h4>
<p>使用torch.eye(n)来创建一个n*n维的单位矩阵</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eye_tensor = torch.eye(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(eye_tensor)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; tensor([[1., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 1.]])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="五从已有的张量创建新的张量"><a class="markdownIt-Anchor" href="#五从已有的张量创建新的张量"></a> <strong>五.从已有的张量创建新的张量</strong></h4>
<p><strong>1. 形状相同：</strong></p>
<p>使用``torch.ones_like`可创建和参数一样Shape的，全为零的tensor</p>
<p>使用``torch.rand_like`可创建和参数一样Shape的，值全为0~1间随机的数字</p>
<p>类似的还有<code>zeros_like</code>和<code>randn_like</code></p>
<p><strong>2.形状和数值都相同</strong></p>
<p>深拷贝：<code>new_tensor=torch.tensor(old_tensor)</code></p>
<p><code>new_tensor=old_tensor.clone()</code></p>
<blockquote>
<p>从ndarray创建的tensor不是深拷贝，而是相同对象的引用</p>
</blockquote>
<h3 id="tensor的运算"><a class="markdownIt-Anchor" href="#tensor的运算"></a> tensor的运算</h3>
<h4 id="基本数学运算"><a class="markdownIt-Anchor" href="#基本数学运算"></a> 基本数学运算</h4>
<p><strong>一.加减乘除</strong></p>
<p>两个tensor的加减乘除得到一个新的<strong>同形状</strong>的tensor，新的 tensor上每一个元素都是参与运算的两个tensor对应元素的加减乘除</p>
<blockquote>
<p>即，<strong>逐元素加减乘除</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tensorA=torch.tensor([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">tensorB=torch.tensor([<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tensorA: &#x27;</span>,tensorA)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tensorB: &#x27;</span>,tensorB)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tensorB+tensorA: &#x27;</span>,tensorB+tensorA)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tensorA-tensorB: &#x27;</span>,-tensorB+tensorA)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tensorB*tensorA: &#x27;</span>,tensorB*tensorA)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tensorB/tensorA: &#x27;</span>,tensorB/tensorA)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">results:</span></span><br><span class="line"><span class="string">tensorA:  tensor([2, 3, 4])</span></span><br><span class="line"><span class="string">tensorB:  tensor([4, 6, 8])</span></span><br><span class="line"><span class="string">tensorB+tensorA:  tensor([ 6,  9, 12])</span></span><br><span class="line"><span class="string">tensorA-tensorB:  tensor([-2, -3, -4])</span></span><br><span class="line"><span class="string">tensorB*tensorA:  tensor([ 8, 18, 32])</span></span><br><span class="line"><span class="string">tensorB/tensorA:  tensor([2., 2., 2.])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><strong>二.矩阵运算</strong></p>
<p><strong>矩阵乘法</strong>：使用<code>torch.matmul</code>或者<code>@</code>运算符</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">b=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a @ b : \n&#x27;</span>,a @ b,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch.matmul(a,b) :\n&#x27;</span>,torch.matmul(a,b))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; a @ b : </span></span><br><span class="line"><span class="string"> tensor([[ 6, 12],</span></span><br><span class="line"><span class="string">        [ 6, 12]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch.matmul(a,b) :</span></span><br><span class="line"><span class="string"> tensor([[ 6, 12],</span></span><br><span class="line"><span class="string">        [ 6, 12]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><strong>矩阵转置</strong>：使用<code>.t()</code>方法对二维张量进行转置，使用<code>.transpose()</code>方法对任意维度张量进行转置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a.transpose(<span class="number">0</span>,<span class="number">1</span>)) <span class="comment">#将第0维和第1维进行交换</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">tensor([[[0.0687, 0.0416, 0.0968, 0.0315],</span></span><br><span class="line"><span class="string">         [0.6092, 0.9713, 0.4944, 0.3970]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.5626, 0.0456, 0.3268, 0.8884],</span></span><br><span class="line"><span class="string">         [0.0375, 0.4542, 0.3374, 0.3793]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.8587, 0.5564, 0.9749, 0.0923],</span></span><br><span class="line"><span class="string">         [0.9446, 0.3608, 0.3371, 0.1169]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>交换维度还有其他的办法：<code>torch.permute()</code>，接受的参数是从0~n-1的整数的一个序列。对于这个序列的第i项，a <sub>i</sub> ,表示把原来的第a<sub>i</sub>维换到了第i维。</p>
<p>比如给定一个三维的张量a=troch.rand(2,3,4)，它的shape为(2,3,4)，如果a.permute(0,2,1)，那么会得到一个shape为(2,4,3)的张量。</p>
<blockquote>
<p><code>permute</code> 只能重新排列已有的维度，而不能改变张量的形状。</p>
</blockquote>
<p><strong>三. 索引，切片，连接</strong></p>
<p>先理解什么是多维的张量，张量不同于向量（或者说向量就是一维的张量），张量的维度和向量的维度不是一个概念，张量的维度和编程中的张量维度是一致的。四维张量是多个三维张量组合而成，三维张量不过是多个二维张量组合而成，以此类推。</p>
<p><strong>1.索引操作</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">tensor([[[0.6739, 0.9918, 0.5762, 0.1026],</span></span><br><span class="line"><span class="string">         [0.4075, 0.6841, 0.9652, 0.4298],</span></span><br><span class="line"><span class="string">         [0.6714, 0.4911, 0.6309, 0.2346]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.5633, 0.0261, 0.7101, 0.6402],</span></span><br><span class="line"><span class="string">         [0.1149, 0.0642, 0.5243, 0.2973],</span></span><br><span class="line"><span class="string">         [0.3718, 0.8068, 0.3164, 0.9270]]])</span></span><br><span class="line"><span class="string">tensor([[0.6739, 0.9918, 0.5762, 0.1026],</span></span><br><span class="line"><span class="string">        [0.4075, 0.6841, 0.9652, 0.4298],</span></span><br><span class="line"><span class="string">        [0.6714, 0.4911, 0.6309, 0.2346]])</span></span><br><span class="line"><span class="string">tensor([0.6739, 0.9918, 0.5762, 0.1026])</span></span><br><span class="line"><span class="string">tensor(0.6739)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>这里上面和多维数组的操作没有什么区别，但是张量还可以直接取出第n维的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(a[:,<span class="number">1</span>]) <span class="comment">#等价于a[:,1,:],得到的shape是原先的shape除去第1维(维度的索引从0开始，这里第一维其实是第二维)</span></span><br><span class="line"><span class="built_in">print</span>(a[:,:,<span class="number">0</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">tensor([[0.4075, 0.6841, 0.9652, 0.4298],</span></span><br><span class="line"><span class="string">        [0.1149, 0.0642, 0.5243, 0.2973]])</span></span><br><span class="line"><span class="string">tensor([[0.6739, 0.4075, 0.6714],</span></span><br><span class="line"><span class="string">        [0.5633, 0.1149, 0.3718]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>如何理解这里的取索引操作呢？这里给出yama的想法：</p>
<ol>
<li>从第0维开始读，如果这一维度的索引已经<strong>指定了</strong>，那么<strong>取对应的元素</strong>，并接着往下读索引；如果索引<strong>没有给出</strong>（包括<code>a[:,1]</code>和<code>a[1]</code>,前者省略了第0维和第1维以后的索引，后者省略了第0维以后所有的索引)那么就<strong>全取</strong></li>
<li>以下面这个例子为例，如果给定了确定的索引，那么取对应的元素（即取对应二维张量）</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string"># 原先的张量，shape为(2,3,4)</span></span><br><span class="line"><span class="string">tensor([[[0.6739, 0.9918, 0.5762, 0.1026],</span></span><br><span class="line"><span class="string">         [0.4075, 0.6841, 0.9652, 0.4298],</span></span><br><span class="line"><span class="string">         [0.6714, 0.4911, 0.6309, 0.2346]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.5633, 0.0261, 0.7101, 0.6402],</span></span><br><span class="line"><span class="string">         [0.1149, 0.0642, 0.5243, 0.2973],</span></span><br><span class="line"><span class="string">         [0.3718, 0.8068, 0.3164, 0.9270]]])</span></span><br><span class="line"><span class="string"># 取出的二维张量，shape为(3,4)</span></span><br><span class="line"><span class="string">tensor([[0.6739, 0.9918, 0.5762, 0.1026],</span></span><br><span class="line"><span class="string">        [0.4075, 0.6841, 0.9652, 0.4298],</span></span><br><span class="line"><span class="string">        [0.6714, 0.4911, 0.6309, 0.2346]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>然后接着读，发现<code>a[0]</code>只指定了第0维的索引，因此，后面默认都是<code>全选</code></p>
<ol start="3">
<li>如果第0维没有指定，但是后面维度指定了索引，下面以这个为例：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(a[:,<span class="number">1</span>]) </span><br><span class="line"><span class="comment">#等价于a[:,1,:],得到的shape是原先的shape除去第1维(维度的索引从0开始，这里第一维其实是第二维)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">tensor([[0.4075, 0.6841, 0.9652, 0.4298],</span></span><br><span class="line"><span class="string">        [0.1149, 0.0642, 0.5243, 0.2973]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>我们可以看到，这里第0维因为没有指定，所以<code>全选</code> （指的最后得到的tensor中，拥有这一维度的所有数据，“原来在这个维度有三个子tensor，现在依旧是这样”，或“这根轴上的长度没有变小”<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/131591991">深度学习中关于张量的阶、轴和形状的解释 | Pytorch系列（二） - 知乎</a>）</p>
<ol start="4">
<li>接着递归地读取索引，即可得到最终的张量。</li>
</ol>
<p><strong>2. 拼接</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]],[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]],[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(torch.cat((a,a),dim=<span class="number">0</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(torch.cat((a,a),dim=<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(torch.cat((a,a),dim=<span class="number">2</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(torch.stack((a,a),dim=<span class="number">0</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(torch.stack((a,a),dim=<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(torch.stack((a,a,a),dim=<span class="number">0</span>).shape)<span class="comment">#也可以多个拼接</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat((a,a,a),dim=<span class="number">0</span>).shape)<span class="comment">#也可以多个拼接</span></span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 3, 2])</span><br><span class="line">torch.Size([6, 3, 2])</span><br><span class="line">torch.Size([3, 6, 2])</span><br><span class="line">torch.Size([3, 3, 4])</span><br><span class="line">torch.Size([2, 3, 3, 2])</span><br><span class="line">torch.Size([3, 2, 3, 2])</span><br><span class="line">torch.Size([3, 3, 3, 2])</span><br><span class="line">torch.Size([9, 3, 2])</span><br></pre></td></tr></table></figure>
<p><code>torch.stack</code>和<code>torch.cat</code>的区别在于前者是把当下dim那一维的数据包在一起，形成一个新的维度；后者是把dim这一层的东西全部家在同一层里面。</p>
<h2 id="自动求导模块"><a class="markdownIt-Anchor" href="#自动求导模块"></a> 自动求导模块</h2>
<h3 id="计算图"><a class="markdownIt-Anchor" href="#计算图"></a> 计算图</h3>
<p>在<strong>向前传播</strong>的过程中，pytorch会记录每一步的操作，便于在<strong>反向传播</strong>的时候求<strong>梯度</strong>，记录的内容就是<strong>计算图</strong></p>
<h3 id="计算梯度"><a class="markdownIt-Anchor" href="#计算梯度"></a> 计算梯度</h3>
<blockquote>
<p>梯度是<strong>损失函数对模型参数的梯度</strong></p>
</blockquote>
<p>我们在forward过程中会获得一个函数，比如<code>y=x**x+3*x+5</code></p>
<p>我们可以借助<code>backward</code>来计算参数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个张量，并设置 requires_grad=True</span></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>, <span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的函数 y = x^2 + 3x + 5</span></span><br><span class="line">y = x**<span class="number">2</span> + <span class="number">3</span>*x + <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 y 对 x 的梯度</span></span><br><span class="line">y.backward(torch.tensor([<span class="number">1.0</span>, <span class="number">1.0</span>]))  <span class="comment"># 传递权重向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x 的梯度:&quot;</span>, x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用梯度计算</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment">#在这个子模块中，不会跟踪梯度计算</span></span><br><span class="line">    z = x**<span class="number">2</span> + <span class="number">3</span>*x + <span class="number">5</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;z 的值:&quot;</span>, z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 detach() 方法 得到一个不需要计算梯度的张量</span></span><br><span class="line">x_detached = x.detach()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_detached 的值:&quot;</span>, x_detached)</span><br></pre></td></tr></table></figure>
<p>如果y是一个标量，那么无需传递权重参数；如果y是一个n维的tensor（或者说x是一个n维的tensor），那么需要传递一个n维数组，代表权重。</p>
<h2 id="数据加载"><a class="markdownIt-Anchor" href="#数据加载"></a> 数据加载</h2>
<h4 id="dataset"><a class="markdownIt-Anchor" href="#dataset"></a> Dataset</h4>
<blockquote>
<p>注意大写<strong>Dataset</strong></p>
</blockquote>
<p>Dataset是一个用于储存和管理数据的一个类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"><span class="comment">#创建属于自己的Dataset类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myDataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,data,labels</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data=data</span><br><span class="line">        <span class="variable language_">self</span>.labels=labels</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx],<span class="variable language_">self</span>.labels[idx]</span><br><span class="line">data=torch.rand(<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">labels=torch.randint(<span class="number">0</span>,<span class="number">2</span>,(<span class="number">10</span>,))</span><br><span class="line"><span class="comment">#注意，这里第三个参数是shape</span></span><br><span class="line">dataset=myDataSet(data,labels)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;idx\tdata\t\t\t\t\tlabel&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">    d,label=dataset[idx]</span><br><span class="line">    <span class="built_in">print</span>(idx,<span class="string">&#x27;\t&#x27;</span>,d,<span class="string">&#x27;\t&#x27;</span>,label)</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">idx	data				               	label</span><br><span class="line">0 	 tensor([0.5844, 0.8253, 0.8348]) 	 tensor(0)</span><br><span class="line">1 	 tensor([0.7215, 0.7513, 0.3788]) 	 tensor(0)</span><br><span class="line">2 	 tensor([0.2878, 0.6035, 0.5831]) 	 tensor(0)</span><br><span class="line">3 	 tensor([0.3502, 0.7822, 0.5622]) 	 tensor(1)</span><br><span class="line">4 	 tensor([0.3010, 0.0765, 0.3608]) 	 tensor(0)</span><br><span class="line">5 	 tensor([0.5522, 0.3278, 0.1177]) 	 tensor(1)</span><br><span class="line">6 	 tensor([0.0602, 0.3381, 0.9306]) 	 tensor(0)</span><br><span class="line">7 	 tensor([0.2770, 0.5383, 0.1920]) 	 tensor(0)</span><br><span class="line">8 	 tensor([0.6101, 0.4523, 0.5425]) 	 tensor(1)</span><br><span class="line">9 	 tensor([0.4292, 0.8549, 0.5172]) 	 tensor(0)</span><br></pre></td></tr></table></figure>
<h3 id="dataloader"><a class="markdownIt-Anchor" href="#dataloader"></a> DataLoader</h3>
<p>DataLoader是加载数据的一个工具，将数据从Dataset中按批次加载，并且支持打乱数据等功能。</p>
<p><strong>可以使用DataLoader将Dataset打包成一个可迭代对象</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myDataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,data,labels</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data=data</span><br><span class="line">        <span class="variable language_">self</span>.labels=labels</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx],<span class="variable language_">self</span>.labels[idx]</span><br><span class="line">data=torch.rand(<span class="number">20</span>,<span class="number">3</span>)</span><br><span class="line">labels=torch.randint(<span class="number">0</span>,<span class="number">2</span>,(<span class="number">20</span>,))</span><br><span class="line">dataset=myDataSet(data,labels)</span><br><span class="line"><span class="built_in">len</span>(dataset)</span><br><span class="line">dataloader=DataLoader(dataset,batch_size=<span class="number">5</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> batch_data,batch_size <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(batch_data)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.7829, 0.9668, 0.0646],</span><br><span class="line">        [0.1576, 0.7564, 0.5376],</span><br><span class="line">        [0.3377, 0.2903, 0.0758],</span><br><span class="line">        [0.4523, 0.6460, 0.5544],</span><br><span class="line">        [0.9514, 0.6436, 0.3744]])</span><br><span class="line">tensor([[0.1227, 0.4072, 0.1857],</span><br><span class="line">        [0.1000, 0.0336, 0.3713],</span><br><span class="line">        [0.4613, 0.8199, 0.5381],</span><br><span class="line">        [0.7966, 0.4351, 0.8256],</span><br><span class="line">        [0.1984, 0.0263, 0.4809]])</span><br><span class="line">tensor([[0.2561, 0.1912, 0.4118],</span><br><span class="line">        [0.2537, 0.5558, 0.3612],</span><br><span class="line">        [0.4839, 0.9967, 0.5584],</span><br><span class="line">        [0.8203, 0.9401, 0.9798],</span><br><span class="line">        [0.3146, 0.0245, 0.3652]])</span><br><span class="line">tensor([[0.0591, 0.4059, 0.8901],</span><br><span class="line">        [0.4426, 0.5236, 0.4988],</span><br><span class="line">        [0.7388, 0.3117, 0.9683],</span><br><span class="line">        [0.7014, 0.3335, 0.4483],</span><br><span class="line">        [0.8727, 0.4119, 0.0646]])</span><br></pre></td></tr></table></figure>
<h2 id="神经网络模块"><a class="markdownIt-Anchor" href="#神经网络模块"></a> 神经网络模块</h2>
<p>推荐阅读：<a target="_blank" rel="noopener" href="https://pytorch.ac.cn/tutorials/beginner/blitz/neural_networks_tutorial.html">神经网络 — PyTorch Tutorials 2.6.0+cu124 文档 - PyTorch 深度学习库</a></p>
<h3 id="构建神经网络torchnnmodule"><a class="markdownIt-Anchor" href="#构建神经网络torchnnmodule"></a> <strong>构建神经网络：torch.nn.Module</strong></h3>
<p><strong>torch.nn.Module</strong>是所有神经网络的基类，所有自定义的网络基类都需要继承这个类</p>
<p><code>torch.nn</code>里面有丰富的内容，其中包括多种网络层和损失函数</p>
<ol>
<li>
<p><strong>线性层</strong><code>torch.nn.Linear(in_features=10,out_features=5)</code>将张量的最后一维（长度为10），映射为长度为5。Linear只会处理最后一维！！！线性变换公式为：<em>y</em>=x * W<sup>T</sup>+b其中 <em>W</em> 是权重矩阵，<em>b</em> 是偏置向量。</p>
</li>
<li>
<p><strong>激活函数</strong>：<code>nn.ReLU</code>,<code>nn.Sigmoid</code>,<code>nn.tanh</code>等</p>
</li>
<li>
<p><strong>损失函数</strong> ：如<code>nn.MSELoss()</code>,<code>nn.CrossEntropyLoss()</code></p>
</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1=nn.Linear(in_features=<span class="number">10</span>,out_features=<span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu=nn.ReLU</span><br><span class="line">        <span class="variable language_">self</span>.fc2=nn.Linear(in_features=<span class="number">5</span>,out_features=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self</span>):</span><br><span class="line">        x=<span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net=MyNet()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
<p>定义一个神经网络，需要定义里面的每一层，以及forward的方法。</p>
<h3 id="评估神经网络损失函数"><a class="markdownIt-Anchor" href="#评估神经网络损失函数"></a> 评估神经网络：损失函数</h3>
<p>评估模型的好坏，我们需要将神经网络的输出和我们预测的输出进行对比，需要用到损失函数。</p>
<p>常见的损失函数有</p>
<ol>
<li>MSELoss: 均方损失函数，用于回归</li>
<li>CrossEntropyLoss交叉熵损失，用于分类问题</li>
<li>BCELoss二院交叉熵损失，用于二分类</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">criterion=nn.BCELoss()<span class="comment">#define the criterion function</span></span><br><span class="line">predictions=net(rawData)</span><br><span class="line">loss=criterion(predictions,targets)</span><br><span class="line"><span class="comment">#compare the predictions and targets </span></span><br><span class="line"><span class="built_in">print</span>(loss.item())</span><br></pre></td></tr></table></figure>
<p>在这里，loss是一个张量，我们可以通过<code>loss.backward()</code>进行<strong>反向传播</strong></p>
<h3 id="优化神经网络torchoptim"><a class="markdownIt-Anchor" href="#优化神经网络torchoptim"></a> <strong>优化神经网络：torch.optim</strong></h3>
<p><code>torch.optim</code> 是 PyTorch 中用于优化模型参数的模块，提供了多种优化算法（如 SGD、Adam 等）。它的主要作用是根据梯度更新模型参数。</p>
<p>在模型训练的过程中，需要不断地进行 清空损失函数的梯度，反向传播，优化参数等步骤</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">optimizer=optim.SGD(net.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()<span class="comment">#每一次都要清空梯度，因为每一次loss调用backward时，都会将梯度进行积累</span></span><br><span class="line">loss.backward()<span class="comment">#反向传播计算梯度</span></span><br><span class="line">optimizer.step()<span class="comment">#优化参数</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>net.paramters()返回一个迭代器，每一个都是网络层的权重和配置等；</p>
<p>lr，learning rate学习率</p>
</blockquote>
<h2 id="保存和加载模型"><a class="markdownIt-Anchor" href="#保存和加载模型"></a> 保存和加载模型</h2>
<p>推荐阅读：[PyTorch | 保存和加载模型 - 知乎](<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82038049#:~:text=%E5%8E%9F%E9%A2%98">https://zhuanlan.zhihu.com/p/82038049#:~:text=原题</a> | SAVING AND LOADING MODELS作者 | Matthew)</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">Saving and Loading Models — PyTorch Tutorials 2.6.0+cu124 documentation</a></p>
<h3 id="save-and-load-the-state_dict"><a class="markdownIt-Anchor" href="#save-and-load-the-state_dict"></a> Save and load the state_dict</h3>
<h4 id="什么是状态字典state_dict"><a class="markdownIt-Anchor" href="#什么是状态字典state_dict"></a> ** 什么是状态字典(state_dict)**</h4>
<p>PyTorch 中，一个模型(<code>torch.nn.Module</code>)的可学习参数(也就是权重和偏置值)是包含在模型参数(<code>model.parameters()</code>)中的，一个状态字典就是一个简单的 Python 的字典，其键值对是每个网络层和其对应的参数张量。模型的状态字典只包含带有可学习参数的网络层（比如卷积层、全连接层等）和注册的缓存（<code>batchnorm</code>的 <code>running_mean</code>）。优化器对象(<code>torch.optim</code>)同样也是有一个状态字典，包含的优化器状态的信息以及使用的超参数。</p>
<blockquote>
<p>Adapted from Zhihu</p>
</blockquote>
<h4 id="加载保存状态字典"><a class="markdownIt-Anchor" href="#加载保存状态字典"></a> <strong>加载/保存状态字典</strong></h4>
<p>保存的代码：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure>
<p>加载的代码：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>
<p>当需要为预测保存一个模型的时候，只需要保存训练模型的可学习参数即可。采用 <code>torch.save()</code> 来保存模型的状态字典的做法可以更方便加载模型，这也是推荐这种做法的原因。</p>
<p>通常会用 <code>.pt</code> 或者 <code>.pth</code> 后缀来保存模型。</p>
<p>记住</p>
<ol>
<li>在进行预测之前，必须调用 <code>model.eval()</code> 方法来将 <code>dropout</code> 和 <code>batch normalization</code> 层设置为验证模型。否则，只会生成前后不一致的预测结果。</li>
<li><code>load_state_dict()</code> 方法必须传入一个字典对象，而不是对象的保存路径，也就是说必须先反序列化字典对象，然后再调用该方法，也是例子中先采用 <code>torch.load()</code> ，而不是直接 <code>model.load_state_dict(PATH)</code></li>
</ol>
<h2 id="项目实战判断一个点在第几个象限多分类"><a class="markdownIt-Anchor" href="#项目实战判断一个点在第几个象限多分类"></a> 项目实战：判断一个点在第几个象限（多分类）</h2>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SorterNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SorterNet,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1=nn.Linear(<span class="number">2</span>,<span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2=nn.Linear(<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc3=nn.Linear(<span class="number">128</span>,<span class="number">4</span>)<span class="comment"># 最终映射得到四个数字，代表四个象限</span></span><br><span class="line">        <span class="variable language_">self</span>.relu=nn.ReLU()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=<span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">sorter=SorterNet()</span><br><span class="line"></span><br><span class="line">optimizer=optim.Adam(sorter.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line">scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">10</span>, gamma=<span class="number">0.1</span>)  <span class="comment"># Reduce LR every 10 epochs by a factor of 0.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,size=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.data=[]</span><br><span class="line">        <span class="variable language_">self</span>.label=[]</span><br><span class="line">        <span class="variable language_">self</span>.size=size</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.size):</span><br><span class="line">            tempArray=np.random.random_sample((<span class="number">2</span>,))*<span class="number">1000</span>-<span class="number">500</span></span><br><span class="line">            <span class="variable language_">self</span>.data.append(tempArray)</span><br><span class="line">            <span class="keyword">if</span>(tempArray[<span class="number">0</span>]&gt;<span class="number">0</span> <span class="keyword">and</span> tempArray[<span class="number">1</span>]&gt;<span class="number">0</span>):</span><br><span class="line">                <span class="variable language_">self</span>.label.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span>(tempArray[<span class="number">0</span>]&lt;<span class="number">0</span> <span class="keyword">and</span> tempArray[<span class="number">1</span>]&gt;<span class="number">0</span>):</span><br><span class="line">                <span class="variable language_">self</span>.label.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">elif</span>(tempArray[<span class="number">0</span>]&lt;<span class="number">0</span> <span class="keyword">and</span> tempArray[<span class="number">1</span>]&lt;<span class="number">0</span>):</span><br><span class="line">                <span class="variable language_">self</span>.label.append(<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>.label.append(<span class="number">3</span>)</span><br><span class="line">        <span class="variable language_">self</span>.label=torch.tensor(<span class="variable language_">self</span>.label,dtype=torch.long)</span><br><span class="line">        <span class="variable language_">self</span>.data=torch.tensor(<span class="variable language_">self</span>.data,dtype=torch.float32)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.size</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx],<span class="variable language_">self</span>.label[idx] <span class="comment">#return both the data and the expected result(label)</span></span><br><span class="line">dataset=myDataset(<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line">dataloader=DataLoader(dataset,batch_size=<span class="number">128</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    loss_sum=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> inputs,labels <span class="keyword">in</span> dataloader:</span><br><span class="line">        results=sorter(inputs)</span><br><span class="line">        loss=criterion(results,labels)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(sorter.parameters(), max_norm=<span class="number">1.0</span>)  <span class="comment"># Clip gradients to avoid exploding gradients</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        loss_sum+=loss.item()</span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">100</span>==<span class="number">99</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch[<span class="subst">&#123;epoch&#125;</span>/1000] loss: <span class="subst">&#123;loss_sum&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>测试代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_model</span>(<span class="params">model, x, y, label</span>):</span><br><span class="line">    <span class="comment"># Convert (x, y) into a tensor and reshape it for the model input</span></span><br><span class="line">    input_tensor = torch.tensor([x, y], dtype=torch.float32).unsqueeze(<span class="number">0</span>)  <span class="comment"># Shape (1, 2)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Pass input through the model</span></span><br><span class="line">    output = model(input_tensor)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get predicted class (index with highest probability)</span></span><br><span class="line">    _, predicted_class = torch.<span class="built_in">max</span>(output, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># If prediction matches the true label, return the updated count</span></span><br><span class="line">    <span class="keyword">if</span> (x &gt;= <span class="number">0</span> <span class="keyword">and</span> y &gt;= <span class="number">0</span> <span class="keyword">and</span> label == predicted_class.item()) <span class="keyword">or</span> \</span><br><span class="line">       (x &lt; <span class="number">0</span> <span class="keyword">and</span> y &gt; <span class="number">0</span> <span class="keyword">and</span> label == predicted_class.item()) <span class="keyword">or</span> \</span><br><span class="line">       (x &lt; <span class="number">0</span> <span class="keyword">and</span> y &lt; <span class="number">0</span> <span class="keyword">and</span> label == predicted_class.item()) <span class="keyword">or</span> \</span><br><span class="line">       (x &gt; <span class="number">0</span> <span class="keyword">and</span> y &lt; <span class="number">0</span> <span class="keyword">and</span> label == predicted_class.item()):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>  <span class="comment"># Correct prediction</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>  <span class="comment"># Incorrect prediction</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_function</span>(<span class="params">model, test_num=<span class="number">1000</span></span>):</span><br><span class="line">    test_input = [[np.random.random_sample(), np.random.random_sample()] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(test_num)]</span><br><span class="line">    test_label = []</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> test_input:</span><br><span class="line">        <span class="keyword">if</span> x &gt;= <span class="number">0</span> <span class="keyword">and</span> y &gt;= <span class="number">0</span>:</span><br><span class="line">            test_label.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> x &lt; <span class="number">0</span> <span class="keyword">and</span> y &gt; <span class="number">0</span>:</span><br><span class="line">            test_label.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> x &lt; <span class="number">0</span> <span class="keyword">and</span> y &lt; <span class="number">0</span>:</span><br><span class="line">            test_label.append(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            test_label.append(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    correct_count = <span class="number">0</span></span><br><span class="line">    test_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> idx, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_input):</span><br><span class="line">        correct_count += test_model(model, x, y, test_label[idx])  <span class="comment"># Count correct predictions</span></span><br><span class="line">        test_count += <span class="number">1</span>  <span class="comment"># Total tests</span></span><br><span class="line"></span><br><span class="line">    correct_rate = correct_count / test_count  <span class="comment"># Calculate accuracy</span></span><br><span class="line">    <span class="keyword">return</span> test_count, correct_count, correct_rate</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the function</span></span><br><span class="line">test_count_sum = <span class="number">0</span></span><br><span class="line">test_correct_sum = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    test_count, correct_count, correct_rate = test_function(sorter)</span><br><span class="line">    test_count_sum += test_count</span><br><span class="line">    test_correct_sum += correct_count</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate overall accuracy</span></span><br><span class="line">rate = test_correct_sum / test_count_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Overall Accuracy: <span class="subst">&#123;rate:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>交互版本：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_quadrant</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># 获取用户输入的坐标</span></span><br><span class="line">    x = <span class="built_in">float</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入x坐标: &quot;</span>))</span><br><span class="line">    y = <span class="built_in">float</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入y坐标: &quot;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将输入的坐标转换为 tensor</span></span><br><span class="line">    input_tensor = torch.tensor([x, y], dtype=torch.float32).unsqueeze(<span class="number">0</span>)  <span class="comment"># Shape (1, 2)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用模型进行预测</span></span><br><span class="line">    output = model(input_tensor)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取预测的类别</span></span><br><span class="line">    _, predicted_class = torch.<span class="built_in">max</span>(output, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 显示预测结果</span></span><br><span class="line">    <span class="keyword">if</span> predicted_class.item() == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;坐标 (<span class="subst">&#123;x&#125;</span>, <span class="subst">&#123;y&#125;</span>) 位于第一象限 (x&gt;0, y&gt;0)&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> predicted_class.item() == <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;坐标 (<span class="subst">&#123;x&#125;</span>, <span class="subst">&#123;y&#125;</span>) 位于第二象限 (x&lt;0, y&gt;0)&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> predicted_class.item() == <span class="number">2</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;坐标 (<span class="subst">&#123;x&#125;</span>, <span class="subst">&#123;y&#125;</span>) 位于第三象限 (x&lt;0, y&lt;0)&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;坐标 (<span class="subst">&#123;x&#125;</span>, <span class="subst">&#123;y&#125;</span>) 位于第四象限 (x&gt;0, y&lt;0)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    predict_quadrant(sorter)  <span class="comment"># 输入并预测某个坐标</span></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://yama-lei.top">Yama-lei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yama-lei.top/posts/3227e790fe57/">https://yama-lei.top/posts/3227e790fe57/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://yama-lei.top" target="_blank">YamaBlog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/yama.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/57133830e9e2/" title="使用瑞莎开发板搭建服务器"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">使用瑞莎开发板搭建服务器</div></div><div class="info-2"><div class="info-item-1">之前心心念念想有一个服务器，今天终于狠下心来买了硬件： 和gemini聊了很久，把相关的知识学了一遍，最后在狗东上面买好了装备： 下面是Gemini的评价  好的，恭喜你！你的硬件配置看起来非常合理且具有性价比，完全能够满足你的需求。  你的硬件配置评价：  Radxa ZERO 3W RK3566 (374元):  评价： 核心设备，RK3566 芯片性能足够处理你的服务器和魔镜任务，Wi-Fi 和蓝牙是 3W 版本的亮点。 建议： 确保你买的是 4GB 或 8GB 内存的版本，如果能有 8GB，会更流畅地运行 Qwen3-0.6B 和多个 Docker 容器。   散热片 (12元):  评价： 必要的配件。RK3566 在高负载下会发热，散热片能有效防止 CPU 过热降频，保证性能稳定。 建议： 安装时确保与芯片接触良好，可以用导热硅脂或导热垫。   240GB SSD 固态硬盘 (117元):  评价： 很好的选择。SSD 相比 microSD 卡有显著的性能和寿命优势，240GB 容量对于操作系统、Docker 镜像、Dify 数据和大部分家庭小规模存储来说非常充裕。性...</div></div></div></a><a class="pagination-related" href="/posts/22f13f9d2720/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2"></div></div><div class="info-2"><div class="info-item-1">读完本文，你可以去力扣拿下如下题目： 28.实现 strStr() ----------- KMP 算法（Knuth-Morris-Pratt 算法）是一个著名的字符串匹配算法，效率很高，但是确实有点复杂。 很多读者抱怨 KMP 算法无法理解，这很正常，想到大学教材上关于 KMP 算法的讲解，也不知道有多少未来的 Knuth、Morris、Pratt 被提前劝退了。有一些优秀的同学通过手推 KMP 算法的过程来辅助理解该算法，这是一种办法，不过本文要从逻辑层面帮助读者理解算法的原理。十行代码之间，KMP 灰飞烟灭。 先在开头约定，本文用 pat 表示模式串，长度为 M，txt 表示文本串，长度为 N。KMP 算法是在 txt 中查找子串 pat，如果存在，返回这个子串的起始索引，否则返回 -1。 为什么我认为 KMP 算法就是个动态规划问题呢，等会再解释。对于动态规划，之前多次强调了要明确 dp 数组的含义，而且同一个问题可能有不止一种定义 dp 数组含义的方法，不同的定义会有不同的解法。 读者见过的 KMP 算法应该是，一波诡异的操作处理 pat 后形成一个一维的数组 next...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/%E5%9B%BE%E7%89%87%E6%AD%A3%E5%9C%A8%E5%8A%A0%E8%BD%BD%E4%B8%AD..." data-lazy-src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/yama.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Yama-lei</div><div class="author-info-description">记录一个NJUCSer的学习</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">105</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yama-lei"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/yama-lei" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:your@email.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss" style="color: #FFA500;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">博客正在建设中...能看就行...😋😋😋</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8Ftensor"><span class="toc-number">1.</span> <span class="toc-text"> 张量：tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor%E7%9A%84%E5%B1%9E%E6%80%A7"><span class="toc-number">1.1.</span> <span class="toc-text"> tensor的属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="toc-number">1.2.</span> <span class="toc-text"> tensor张量的创建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80-%E7%94%A8torchtensor%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.1.</span> <span class="toc-text"> 一. 用torch.tensor()方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E4%BD%BF%E7%94%A8%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%E5%88%9B%E5%BB%BA%E7%89%B9%E6%AE%8A%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 二. 使用内置函数创建特殊形状的函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B-%E5%88%9B%E5%BB%BA%E5%8D%95%E4%BD%8D%E7%9F%A9%E9%98%B5"><span class="toc-number">1.2.3.</span> <span class="toc-text"> 四. 创建单位矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94%E4%BB%8E%E5%B7%B2%E6%9C%89%E7%9A%84%E5%BC%A0%E9%87%8F%E5%88%9B%E5%BB%BA%E6%96%B0%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-number">1.2.4.</span> <span class="toc-text"> 五.从已有的张量创建新的张量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor%E7%9A%84%E8%BF%90%E7%AE%97"><span class="toc-number">1.3.</span> <span class="toc-text"> tensor的运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number">1.3.1.</span> <span class="toc-text"> 基本数学运算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E6%A8%A1%E5%9D%97"><span class="toc-number">2.</span> <span class="toc-text"> 自动求导模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">2.1.</span> <span class="toc-text"> 计算图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="toc-number">2.2.</span> <span class="toc-text"> 计算梯度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">3.</span> <span class="toc-text"> 数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dataset"><span class="toc-number">3.0.1.</span> <span class="toc-text"> Dataset</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dataloader"><span class="toc-number">3.1.</span> <span class="toc-text"> DataLoader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97"><span class="toc-number">4.</span> <span class="toc-text"> 神经网络模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ctorchnnmodule"><span class="toc-number">4.1.</span> <span class="toc-text"> 构建神经网络：torch.nn.Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.</span> <span class="toc-text"> 评估神经网络：损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ctorchoptim"><span class="toc-number">4.3.</span> <span class="toc-text"> 优化神经网络：torch.optim</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text"> 保存和加载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#save-and-load-the-state_dict"><span class="toc-number">5.1.</span> <span class="toc-text"> Save and load the state_dict</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%8A%B6%E6%80%81%E5%AD%97%E5%85%B8state_dict"><span class="toc-number">5.1.1.</span> <span class="toc-text"> ** 什么是状态字典(state_dict)**</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E4%BF%9D%E5%AD%98%E7%8A%B6%E6%80%81%E5%AD%97%E5%85%B8"><span class="toc-number">5.1.2.</span> <span class="toc-text"> 加载&#x2F;保存状态字典</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E5%88%A4%E6%96%AD%E4%B8%80%E4%B8%AA%E7%82%B9%E5%9C%A8%E7%AC%AC%E5%87%A0%E4%B8%AA%E8%B1%A1%E9%99%90%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-number">6.</span> <span class="toc-text"> 项目实战：判断一个点在第几个象限（多分类）</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/66b650927646/" title="多维数组">多维数组</a><time datetime="2025-09-30T16:00:00.000Z" title="发表于 2025-10-01 00:00:00">2025-10-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/66c438509deb/" title="多模态综述">多模态综述</a><time datetime="2025-09-30T13:14:40.000Z" title="发表于 2025-09-30 21:14:40">2025-09-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/54b78cdb42c3/" title="无标题">无标题</a><time datetime="2025-09-30T01:58:28.349Z" title="发表于 2025-09-30 09:58:28">2025-09-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/0ac3e125560f/" title="Riscv介绍">Riscv介绍</a><time datetime="2025-09-29T16:00:00.000Z" title="发表于 2025-09-30 00:00:00">2025-09-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="ft"><div class="ft-item-1"><div class="t-top"><div class="t-t-l"><p class="ft-t t-l-t">格言🧬</p><div class="bg-ad"><div>记录一个NJUCSer的学习历程 · 代码改变世界，技术创造未来</div><div class="btn-xz-box"><a class="btn-xz" href="/about/">了解更多</a></div></div></div></div></div><div class="ft-item-2"><p class="ft-t">推荐友链⌛</p><div class="ft-img-group"><div class="img-group-item"><a href="/links/" title="广告位招租"><img src="/%E5%9B%BE%E7%89%87%E6%AD%A3%E5%9C%A8%E5%8A%A0%E8%BD%BD%E4%B8%AD..." data-lazy-src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/zhaozu.jpg" alt="广告位招租"/></a></div><div class="img-group-item"><a href="/links/" title="更多友联"><img src="/%E5%9B%BE%E7%89%87%E6%AD%A3%E5%9C%A8%E5%8A%A0%E8%BD%BD%E4%B8%AD..." data-lazy-src="https://yamapicgo.oss-cn-nanjing.aliyuncs.com/picgoImage/zhaozu.jpg" alt="友链"/></a></div></div></div></div><div class="copyright"><span><b>&copy;2025</b></span><span><b>&nbsp;&nbsp;By Yama-lei</b></span></div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Less is more · © YamaBlog</div><div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo"><img src="/%E5%9B%BE%E7%89%87%E6%AD%A3%E5%9C%A8%E5%8A%A0%E8%BD%BD%E4%B8%AD..." data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat-square" alt="Hexo"/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly"><img src="/%E5%9B%BE%E7%89%87%E6%AD%A3%E5%9C%A8%E5%8A%A0%E8%BD%BD%E4%B8%AD..." data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat-square" alt="Butterfly"/></a><a class="github-badge" target="_blank" href="https://github.com/yama-lei" style="margin-inline:5px" title="源码托管于GitHub"><img src="/%E5%9B%BE%E7%89%87%E6%AD%A3%E5%9C%A8%E5%8A%A0%E8%BD%BD%E4%B8%AD..." data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat-square" alt="GitHub"/></a></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script src="/js/custom.js"></script><script src="/js/footer-enhance.js"></script><script src="/js/homepage-enhance.js"></script><script defer src="/js/runtime.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>